{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schinavro/anaconda3/envs/simple/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from mlip.pes import PotentialNeuralNet\n",
    "from mlip.reann import REANN, compress_symbols\n",
    "\n",
    "def gen_module(species=None, moduledict=None, modulelist=None, nmax=2, lmax=10, norb=100, loop=2, rcut=6.0, device='cpu'):\n",
    "\n",
    "    encode, decode, numbers = compress_symbols(species)\n",
    "    species = list(set(numbers))\n",
    "    reann = REANN(species, modulelist=modulelist, nmax=nmax, lmax=lmax, norb=norb, loop=loop, device=device, rcut=rcut)\n",
    "    \n",
    "    return PotentialNeuralNet(reann, moduledict, species)\n",
    "model = gen_module(species=[29], lmax = 2, nmax = 15, loop = 2, device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True torch.Size([1, 15])\n",
      "True torch.Size([1, 15])\n",
      "True torch.Size([1, 15])\n",
      "True torch.Size([3, 3, 15, 100])\n",
      "True torch.Size([128, 100])\n",
      "True torch.Size([128])\n",
      "True torch.Size([15, 128])\n",
      "True torch.Size([15])\n",
      "True torch.Size([128, 100])\n",
      "True torch.Size([128])\n",
      "True torch.Size([15, 128])\n",
      "True torch.Size([15])\n"
     ]
    }
   ],
   "source": [
    "for parameter in model.parameters():\n",
    "    print(parameter.requires_grad, parameter.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nomenclature\n",
    "# SPEC-PECriG(symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients)\n",
    "\n",
    "import torch as tc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BPTypeDataset(Dataset):\n",
    "    \n",
    "    \"\"\"Behler Parrinello Type datasets\n",
    "    Indexing should be done in the unit of crystal, a set of atom used in one calculation.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        symbols: List\n",
    "        positions: List\n",
    "        energies: List\n",
    "        cells: List\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients):\n",
    "        self.symbols = symbols\n",
    "        self.positions = positions\n",
    "        self.energies = energies\n",
    "        self.cells = cells\n",
    "        self.pbcs = pbcs\n",
    "        self.energyidx = energyidx\n",
    "        self.crystalidx = crystalidx\n",
    "        self.gradients = gradients\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.energies)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.symbols[idx], self.positions[idx], self.energies[idx], self.cells[idx], self.pbcs[idx], self.energyidx[idx], self.crystalidx[idx], self.gradients[idx]\n",
    "\n",
    "    \n",
    "def concate(batch, device='cpu'):\n",
    "    cat = lambda x: tc.from_numpy(np.concatenate(x))\n",
    "    \n",
    "    symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients = [], [], [], [], [], [], [], []\n",
    "    for data in batch:\n",
    "        symbol, position, energy, cell, pbc, energyid, crystalid, gradient = data\n",
    "        symbols.append(symbol)\n",
    "        positions.append(position)\n",
    "        energies.append(energy)\n",
    "        cells.append(cell[None])\n",
    "        pbcs.append(pbc[None])      \n",
    "        energyidx.append(energyid)\n",
    "        crystalidx.append(crystalid)\n",
    "        gradients.append(gradient)\n",
    "\n",
    "    return (cat(symbols), cat(positions).to(device=device).requires_grad_(True), \n",
    "            tc.tensor(energies).to(device=device), cat(cells).to(device=device).requires_grad_(True), \n",
    "            cat(pbcs), tc.tensor(energyidx).to(device=device), cat(crystalidx).to(device=device), cat(gradients).to(device=device))\n",
    "\n",
    "import numpy as np\n",
    "from pymatgen.core import Structure\n",
    "from monty.serialization import loadfn\n",
    "\n",
    "def get_dataloader(location, symbol, number, name, batch_size=10):\n",
    "\n",
    "    data = loadfn(location + symbol + '/' + name)\n",
    "    encode, decode, numbers = compress_symbols([number])\n",
    "\n",
    "    #data[0]['structure'].cart_coords;\n",
    "    #data[0]['structure'].lattice.matrix;\n",
    "    #data[0]['outputs']['forces'];\n",
    "    #data[0]['structure'].lattice.pbc;\n",
    "    #data[0]['num_atoms']\n",
    "    symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients  = [], [], [], [], [], [], [], []\n",
    "    for idx, d in enumerate(data):   \n",
    "#        if d['outputs']['energy'] > -400:\n",
    "#            continue\n",
    "        symbols.append([encode[n] for n in d['structure'].atomic_numbers])\n",
    "        positions.append(d['structure'].cart_coords)\n",
    "        energies.append(d['outputs']['energy'])\n",
    "        cells.append(d['structure'].lattice.matrix)\n",
    "        pbcs.append(np.array(d['structure'].lattice.pbc))\n",
    "        energyidx.append(idx)\n",
    "        crystalidx.append([idx] * data[idx]['num_atoms'])\n",
    "        gradients.append(-np.array(d['outputs']['forces']))\n",
    "\n",
    "    imgdataset = BPTypeDataset(symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients)\n",
    "    \n",
    "    return imgdataset, DataLoader(imgdataset, batch_size=batch_size, shuffle=True, collate_fn=concate)\n",
    "\n",
    "#location = \"/home01/x2419a03/libCalc/mlip/data/\"\n",
    "location = \"/home/schinavro/libCalc/mlip/data/\"\n",
    "symbol = 'Cu'\n",
    "number = 29\n",
    "imgdataset, dataloader = get_dataloader(location, symbol, number, 'training.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEFLoss:\n",
    "    def __init__(self, muE=1., muF=1.):\n",
    "        self.muE = muE\n",
    "        self.muF = muF\n",
    "    def __call__(self, predE, predF, y, dy):\n",
    "        #_, NA = tc.unique_consecutive(crystalidx, return_counts=True)\n",
    "        #self.NTA = len(dy)\n",
    "        #self.lossE = tc.sqrt(tc.sum(((y - predE)/ NA)**2) / len(y))\n",
    "        #self.lossG = tc.sqrt(tc.sum((predF - dy)**2) / self.NTA)\n",
    "        self.lossE = tc.sum((y - predE)**2)\n",
    "        self.lossG = tc.sum((dy - predF)**2)\n",
    "        return self.muE * self.lossE + self.muF * self.lossG\n",
    "\n",
    "\n",
    "class Normalizer(object):\n",
    "    \"\"\"Normalize a Tensor and restore it later. \"\"\"\n",
    "\n",
    "    def __init__(self, tensor, N, device='cuda'):\n",
    "        \"\"\"tensor is taken as a sample to calculate the mean and std\"\"\"\n",
    "        self.mean = tc.mean(tensor).to(device=device) / N\n",
    "        self.std = tc.std(tensor).to(device=device)\n",
    "\n",
    "    def norm(self, tensor, N):\n",
    "        #return (tensor - self.mean) / self.std\n",
    "        return tensor - N * self.mean\n",
    "\n",
    "    def denorm(self, normed_tensor, N):\n",
    "        #return normed_tensor * self.std + self.mean\n",
    "        return normed_tensor + self.mean\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {'mean': self.mean,\n",
    "                'std': self.std}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.mean = state_dict['mean']\n",
    "        self.std = state_dict['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([38242, 40, 20]) torch.Size([38242])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (13) must match the size of tensor b (40) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 168>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m     modulelist\u001b[38;5;241m.\u001b[39mappend(Gj(descdict, species\u001b[38;5;241m=\u001b[39mspecies, nmax\u001b[38;5;241m=\u001b[39mnmax))\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sym, num \u001b[38;5;129;01min\u001b[39;00m tables\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 169\u001b[0m     \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43msym\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlmax\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlmax\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrcut\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrcut\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmoduledict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmoduledict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodulelist\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodulelist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdouble\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(symbol, number, moduledict, modulelist, lmax, nmax, norb, rcut, loop, batch_size, location, device)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5000\u001b[39m):\n\u001b[1;32m     82\u001b[0m     loss_fn \u001b[38;5;241m=\u001b[39m MSEFLoss(muF\u001b[38;5;241m=\u001b[39mmuF)\n\u001b[0;32m---> 83\u001b[0m     lossE, lossG \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     86\u001b[0m         tc\u001b[38;5;241m.\u001b[39msave(model\u001b[38;5;241m.\u001b[39mstate_dict(), \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./20220930/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m symbol \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_weights_\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m t)\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer, normalizer, device)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     34\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 36\u001b[0m _, pred, predG \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcells\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menergyidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrystalidx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(gradients)\n\u001b[1;32m     40\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, predG, energies, gradients)\n",
      "File \u001b[0;32m~/anaconda3/envs/simple/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/simple/lib/python3.8/site-packages/mlip/pes.py:67\u001b[0m, in \u001b[0;36mPotentialNeuralNet.forward\u001b[0;34m(self, symbols, positions, cells, pbcs, energyidx, crystalidx, cutoff)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     63\u001b[0m \n\u001b[1;32m     64\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Descriptor calculation\u001b[39;00m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# NTA x 3 -> NTA x D\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcells\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbcs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menergyidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrystalidx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m positionsidx \u001b[38;5;241m=\u001b[39m tc\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(positions))\n\u001b[1;32m     70\u001b[0m new_energies, new_positionsidx \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/simple/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/simple/lib/python3.8/site-packages/mlip/reann.py:409\u001b[0m, in \u001b[0;36mREANN.forward\u001b[0;34m(self, symbols, positions, cells, pbcs, energyidx, crystalidx)\u001b[0m\n\u001b[1;32m    407\u001b[0m params \u001b[38;5;241m=\u001b[39m (device, dtype, NTA, NO, nmax)\n\u001b[1;32m    408\u001b[0m \u001b[38;5;66;03m# NTA x norb\u001b[39;00m\n\u001b[0;32m--> 409\u001b[0m ρ \u001b[38;5;241m=\u001b[39m \u001b[43mget_density\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWln\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCsn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFxyz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop):\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;66;03m# NTAxO -> NTAxnmax\u001b[39;00m\n\u001b[1;32m    412\u001b[0m     Csn \u001b[38;5;241m=\u001b[39m Csn \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgj[i](ρ, symbols)\n",
      "File \u001b[0;32m~/anaconda3/envs/simple/lib/python3.8/site-packages/mlip/reann.py:257\u001b[0m, in \u001b[0;36mget_density\u001b[0;34m(Wln, Csn, Fxyz, iidx, jidx, device, dtype, NTA, NO, nmax)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mprint\u001b[39m(cjFxyz\u001b[38;5;241m.\u001b[39mshape, iidx\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m    256\u001b[0m \u001b[38;5;66;03m# NN x NO x nmax -> NTA x NO x nmax\u001b[39;00m\n\u001b[0;32m--> 257\u001b[0m bnl \u001b[38;5;241m=\u001b[39m \u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNTA\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mNO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnmax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_add\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m                                        \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcjFxyz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;66;03m# NTAxNOx(nmax)x1 x 1xNOx(nmax)x(norb) -> NTAxNOx(nmax)xnorb\u001b[39;00m\n\u001b[1;32m    261\u001b[0m \u001b[38;5;66;03m#  -> NTAxNOxnorb -> NTAxnorb\u001b[39;00m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tc\u001b[38;5;241m.\u001b[39msum(tc\u001b[38;5;241m.\u001b[39msum(bnl[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m Wln[\u001b[38;5;28;01mNone\u001b[39;00m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (13) must match the size of tensor b (40) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "import torch as tc\n",
    "from torch.autograd import grad\n",
    "\n",
    "def test(dataloader, model, loss_fn, normalizer, device='cuda'):\n",
    "        \n",
    "    lossE, lossG, NTA = 0., 0., 0\n",
    "    model.eval()\n",
    "    for batch, _ in enumerate(dataloader):\n",
    "\n",
    "        symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients  = _\n",
    "#        with torch.no_grad():\n",
    "        _, pred, predG = model(symbols, positions, cells, pbcs, energyidx, crystalidx)\n",
    "        A = len(gradients)\n",
    "        \n",
    "#        loss = loss_fn(normalizer.denorm(pred, A), predG, energies, gradients)\n",
    "        loss = loss_fn(pred, predG, energies, gradients)\n",
    "        \n",
    "        NTA += A\n",
    "        lossE += loss_fn.lossE.item()\n",
    "        lossG += loss_fn.lossG.item()\n",
    "\n",
    "    return lossE / NTA, lossG / NTA\n",
    "    \n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, normalizer, device='cuda'):\n",
    "    \n",
    "    lossE, lossG, NTA = 0., 0., 0\n",
    "    model.train()\n",
    "    for batch, _ in enumerate(dataloader):\n",
    "\n",
    "        symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients  = _\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        _, pred, predG = model(symbols, positions, cells, pbcs, energyidx, crystalidx)\n",
    "        \n",
    "        A = len(gradients)\n",
    "        \n",
    "        loss = loss_fn(pred, predG, energies, gradients)\n",
    "#        loss = loss_fn(pred, predG, normalizer.norm(energies, A), 27416.)\n",
    "        \n",
    "        loss.requires_grad_(True)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        NTA += A\n",
    "        lossE += loss_fn.lossE.item()\n",
    "        lossG += loss_fn.lossG.item()\n",
    "\n",
    "    return lossE / NTA, lossG / NTA\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def run(symbol, number, moduledict=None, modulelist=None, lmax=2, nmax=15, norb=100, rcut=6., loop=2, batch_size=30, \n",
    "        location=\"/home01/x2419a03/libCalc/mlip/data/\", \n",
    "        device='cpu'):\n",
    "    species = [number]\n",
    "    \n",
    "    log_dir = './20220930/' + symbol + '_log'\n",
    "    \n",
    "    model = gen_module(species=species, moduledict=moduledict, modulelist=modulelist, \n",
    "                       lmax=lmax, nmax=nmax, loop=loop, rcut=rcut, device=device, norb=norb)\n",
    "#    model = nn.DataParallel(model)\n",
    "    ####\n",
    "#    model.load_state_dict(tc.load('/scratch/x2419a03/workspace/20220803/Cu_weights_100.pt'))\n",
    "#    model.desc.rcut = tc.tensor([rcut]).double().to(device=device)\n",
    "    ####\n",
    "    \n",
    "    imgdataset, dataloader = get_dataloader(location, symbol, number, 'training.json', batch_size=batch_size)\n",
    "    test_imgdataset, test_dataloader = get_dataloader(location, symbol, number, 'test.json', batch_size=batch_size)\n",
    "\n",
    "\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    normalizer = Normalizer(tc.tensor(imgdataset.energies).double(), len(imgdataset.symbols),\n",
    "                           device=device)\n",
    "\n",
    "    lr, muF = 1e-1, 3\n",
    "    for t in range(5000):\n",
    "        loss_fn = MSEFLoss(muF=muF)\n",
    "        lossE, lossG = train(dataloader, model, loss_fn, \n",
    "                             tc.optim.Adam(model.parameters(), lr=lr), normalizer)\n",
    "        if t % 10 == 0:\n",
    "            tc.save(model.state_dict(), './20220930/' + symbol + '_weights_%d.pt' % t)\n",
    "        loss = lossE + lossG\n",
    "        \n",
    "\n",
    "        # 30 meV\n",
    "        if loss < 3e-2:\n",
    "            lr = 1e-5\n",
    "            muF = 1\n",
    "        # 100 meV\n",
    "        elif loss < 1e-2:\n",
    "            lr = 1e-4\n",
    "            muF = 1.5\n",
    "        # 300 meV\n",
    "        elif loss < 3e-1:\n",
    "            lr = 1e-3\n",
    "            muF = 2\n",
    "        # 1 eV\n",
    "        elif loss < 1:\n",
    "            lr = 1e-2\n",
    "            muF = 2.5\n",
    "            \n",
    "#         # 30 meV\n",
    "#         if lossG < 3e-2:\n",
    "#             muF = 1\n",
    "#         # 100 meV\n",
    "#         if lossG < 1e-2:\n",
    "#             muF = 3\n",
    "#         # 300 meV\n",
    "#         elif lossG < 3e-2:\n",
    "#             muF = 10\n",
    "#         # 1 eV meV\n",
    "#         elif lossG < 1e-1:\n",
    "#             muF = 30\n",
    "\n",
    "        writer.add_scalar('training RMSE-E (eV/atom)', lossE, t)\n",
    "        writer.add_scalar('training RMSE-F (eV/A)', lossG, t)\n",
    "        \n",
    "        test_lossE, test_lossG = test(test_dataloader, model, loss_fn, normalizer)\n",
    "        writer.add_scalar('test RMSE-E (eV/atom)', test_lossE, t)\n",
    "        writer.add_scalar('test RMSE-F (eV/A)', test_lossG, t)\n",
    "        \n",
    "        print(t, \": {0:<10.2f} {1:<10.2f} {2:<10.2f} {3:<10.2f}\".format(lossE * 1000, lossG* 1000, test_lossE* 1000, test_lossG* 1000))\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    tc.cuda.empty_cache()\n",
    "    \n",
    "\n",
    "#tables = {'Cu': 29, 'Ge': 32, 'Li': 3, 'Mo': 42, 'Ni':28, 'Si': 14}\n",
    "tables = {'Cu': 29}\n",
    "\n",
    "\n",
    "from mlip.reann import Gj\n",
    "\n",
    "device='cpu'\n",
    "\n",
    "species = [0]\n",
    "nmax = 20\n",
    "loop = 3\n",
    "rcut = 3.\n",
    "norb = 10\n",
    "lmax = 3\n",
    "\n",
    "moduledict = nn.ModuleDict()\n",
    "for spe in species:\n",
    "    moduledict[str(spe)] = nn.Sequential(\n",
    "        nn.Linear(norb, 128),\n",
    "        nn.Softplus(),\n",
    "        nn.Linear(128, 1),\n",
    "    )\n",
    "    \n",
    "modulelist = nn.ModuleList()\n",
    "for j in range(loop):\n",
    "    descdict = nn.ModuleDict()\n",
    "    for spe in species:\n",
    "        descdict[str(spe)] = nn.Sequential(\n",
    "                nn.Linear(norb, 128),\n",
    "                 nn.Softplus(),\n",
    "                nn.Linear(128, nmax)\n",
    "        )\n",
    "    modulelist.append(Gj(descdict, species=species, nmax=nmax))\n",
    "\n",
    "for sym, num in tables.items():\n",
    "    run(sym, num, device=device, nmax=nmax, lmax=lmax, norb=norb, loop=loop, rcut=rcut, \n",
    "        location=location,\n",
    "        moduledict=moduledict.double().to(device=device),\n",
    "        modulelist=modulelist.double().to(device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello I am JaeHwan shim from jaejun Yu's group. \n",
    "\n",
    "I am going to explain about the Recursive embdding atomic nerual network. Recursive embedding neural network is neural network interatomic potential model which has foreground on the physics. \n",
    "\n",
    "Contents of my presentation is\n",
    "\n",
    "1. REANN 적용 현황\n",
    "\n",
    "single atoms\n",
    "\n",
    "\n",
    "\n",
    "Results \n",
    "\n",
    "Results and discussion \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Cu': 29, 'Ge': 32, 'Li': 3, 'Mo': 42, 'Ni':28, 'Si': 14"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple",
   "language": "python",
   "name": "simple"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
