{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/x2419a03/.conda/envs/simple/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from mlip.pes import PotentialNeuralNet\n",
    "from mlip.reann import REANN, compress_symbols\n",
    "\n",
    "def gen_module(species=None, moduledict=None, modulelist=None, nmax=2, lmax=10, loop=2, rcut=6.0, device='cpu'):\n",
    "\n",
    "    encode, decode, numbers = compress_symbols(species)\n",
    "    species = list(set(numbers))\n",
    "    reann = REANN(species, modulelist=modulelist, nmax=nmax, lmax=lmax, loop=loop, device=device, rcut=rcut)\n",
    "    \n",
    "    return PotentialNeuralNet(reann, moduledict, species)\n",
    "model = gen_module(species=[29], lmax = 2, nmax = 15, loop = 2, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nomenclature\n",
    "# SPEC-PECriG(symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients)\n",
    "\n",
    "import torch as tc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BPTypeDataset(Dataset):\n",
    "    \n",
    "    \"\"\"Behler Parrinello Type datasets\n",
    "    Indexing should be done in the unit of crystal, a set of atom used in one calculation.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        symbols: List\n",
    "        positions: List\n",
    "        energies: List\n",
    "        cells: List\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients):\n",
    "        self.symbols = symbols\n",
    "        self.positions = positions\n",
    "        self.energies = energies\n",
    "        self.cells = cells\n",
    "        self.pbcs = pbcs\n",
    "        self.energyidx = energyidx\n",
    "        self.crystalidx = crystalidx\n",
    "        self.gradients = gradients\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.energies)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.symbols[idx], self.positions[idx], self.energies[idx], self.cells[idx], self.pbcs[idx], self.energyidx[idx], self.crystalidx[idx], self.gradients[idx]\n",
    "\n",
    "    \n",
    "def concate(batch, device='cuda'):\n",
    "    cat = lambda x: tc.from_numpy(np.concatenate(x))\n",
    "    \n",
    "    symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients = [], [], [], [], [], [], [], []\n",
    "    for data in batch:\n",
    "        symbol, position, energy, cell, pbc, energyid, crystalid, gradient = data\n",
    "        symbols.append(symbol)\n",
    "        positions.append(position)\n",
    "        energies.append(energy)\n",
    "        cells.append(cell[None])\n",
    "        pbcs.append(pbc[None])      \n",
    "        energyidx.append(energyid)\n",
    "        crystalidx.append(crystalid)\n",
    "        gradients.append(gradient)\n",
    "\n",
    "    return (cat(symbols), cat(positions).to(device=device).requires_grad_(True), \n",
    "            tc.tensor(energies).to(device=device), cat(cells).to(device=device).requires_grad_(True), \n",
    "            cat(pbcs), tc.tensor(energyidx).to(device=device), cat(crystalidx).to(device=device), cat(gradients).to(device=device))\n",
    "\n",
    "import numpy as np\n",
    "from pymatgen.core import Structure\n",
    "from monty.serialization import loadfn\n",
    "\n",
    "def get_dataloader(location, symbol, number, name, batch_size=10):\n",
    "\n",
    "    data = loadfn(location + symbol + '/' + name)\n",
    "    encode, decode, numbers = compress_symbols([number])\n",
    "\n",
    "    #data[0]['structure'].cart_coords;\n",
    "    #data[0]['structure'].lattice.matrix;\n",
    "    #data[0]['outputs']['forces'];\n",
    "    #data[0]['structure'].lattice.pbc;\n",
    "    #data[0]['num_atoms']\n",
    "    symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients  = [], [], [], [], [], [], [], []\n",
    "    for idx, d in enumerate(data):   \n",
    "#        if d['outputs']['energy'] > -400:\n",
    "#            continue\n",
    "        symbols.append([encode[n] for n in d['structure'].atomic_numbers])\n",
    "        positions.append(d['structure'].cart_coords)\n",
    "        energies.append(d['outputs']['energy'])\n",
    "        cells.append(d['structure'].lattice.matrix)\n",
    "        pbcs.append(np.array(d['structure'].lattice.pbc))\n",
    "        energyidx.append(idx)\n",
    "        crystalidx.append([idx] * data[idx]['num_atoms'])\n",
    "        gradients.append(-np.array(d['outputs']['forces']))\n",
    "\n",
    "    imgdataset = BPTypeDataset(symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients)\n",
    "    \n",
    "    return imgdataset, DataLoader(imgdataset, batch_size=batch_size, shuffle=True, collate_fn=concate)\n",
    "\n",
    "location = \"/home01/x2419a03/libCalc/mlip/data/\"\n",
    "symbol = 'Cu'\n",
    "number = 29\n",
    "imgdataset, dataloader = get_dataloader(location, symbol, number, 'training.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEFLoss:\n",
    "    def __init__(self, muE=1., muF=1.):\n",
    "        self.muE = muE\n",
    "        self.muF = muF\n",
    "    def __call__(self, predE, predF, y, dy):\n",
    "        #_, NA = tc.unique_consecutive(crystalidx, return_counts=True)\n",
    "        #self.NTA = len(dy)\n",
    "        #self.lossE = tc.sqrt(tc.sum(((y - predE)/ NA)**2) / len(y))\n",
    "        #self.lossG = tc.sqrt(tc.sum((predF - dy)**2) / self.NTA)\n",
    "        self.lossE = tc.sum((y - predE)**2)\n",
    "        self.lossG = tc.sum((dy - predF)**2)\n",
    "        return self.muE * self.lossE + self.muF * self.lossG\n",
    "\n",
    "\n",
    "class Normalizer(object):\n",
    "    \"\"\"Normalize a Tensor and restore it later. \"\"\"\n",
    "\n",
    "    def __init__(self, tensor, N, device='cuda'):\n",
    "        \"\"\"tensor is taken as a sample to calculate the mean and std\"\"\"\n",
    "        self.mean = tc.mean(tensor).to(device=device) / N\n",
    "        self.std = tc.std(tensor).to(device=device)\n",
    "\n",
    "    def norm(self, tensor, N):\n",
    "        #return (tensor - self.mean) / self.std\n",
    "        return tensor - N * self.mean\n",
    "\n",
    "    def denorm(self, normed_tensor, N):\n",
    "        #return normed_tensor * self.std + self.mean\n",
    "        return normed_tensor + self.mean\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {'mean': self.mean,\n",
    "                'std': self.std}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.mean = state_dict['mean']\n",
    "        self.std = state_dict['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 1823423.38 589139.34  205110.91  53293.34  \n",
      "1 : 1243499.25 10172.38   993286.02  2279.52   \n",
      "2 : 503826.81  2447.39    339434.77  2255.62   \n",
      "3 : 397229.08  2705.38    269910.56  5069.81   \n"
     ]
    }
   ],
   "source": [
    "import torch as tc\n",
    "from torch.autograd import grad\n",
    "\n",
    "def test(dataloader, model, loss_fn, normalizer, device='cuda'):\n",
    "        \n",
    "    lossE, lossG, NTA = 0., 0., 0\n",
    "    model.eval()\n",
    "    for batch, _ in enumerate(dataloader):\n",
    "\n",
    "        symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients  = _\n",
    "#        with torch.no_grad():\n",
    "        _, pred, predG = model(symbols, positions, cells, pbcs, energyidx, crystalidx)\n",
    "        A = len(gradients)\n",
    "        \n",
    "#        loss = loss_fn(normalizer.denorm(pred, A), predG, energies, gradients)\n",
    "        loss = loss_fn(pred, predG, energies, gradients)\n",
    "        \n",
    "        NTA += A\n",
    "        lossE += loss_fn.lossE.item()\n",
    "        lossG += loss_fn.lossG.item()\n",
    "\n",
    "    return lossE / NTA, lossG / NTA\n",
    "    \n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, normalizer, device='cuda'):\n",
    "    \n",
    "    lossE, lossG, NTA = 0., 0., 0\n",
    "    model.train()\n",
    "    for batch, _ in enumerate(dataloader):\n",
    "\n",
    "        symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients  = _\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        _, pred, predG = model(symbols, positions, cells, pbcs, energyidx, crystalidx)\n",
    "        \n",
    "        A = len(gradients)\n",
    "        \n",
    "        loss = loss_fn(pred, predG, energies, gradients)\n",
    "#        loss = loss_fn(pred, predG, normalizer.norm(energies, A), 27416.)\n",
    "        \n",
    "        loss.requires_grad_(True)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        NTA += A\n",
    "        lossE += loss_fn.lossE.item()\n",
    "        lossG += loss_fn.lossG.item()\n",
    "\n",
    "    return lossE / NTA, lossG / NTA\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def run(symbol, number, moduledict=None, modulelist=None, lmax=2, nmax=15, rcut=6., loop=2, batch_size=30, \n",
    "        location=\"/home01/x2419a03/libCalc/mlip/data/\", \n",
    "        device='cpu'):\n",
    "    species = [number]\n",
    "    \n",
    "    log_dir = './20220804/' + symbol + '_log'\n",
    "    \n",
    "    model = gen_module(species=species, moduledict=moduledict, modulelist=modulelist, \n",
    "                       lmax=lmax, nmax=nmax, loop=loop, rcut=rcut, device=device)\n",
    "#    model = nn.DataParallel(model)\n",
    "    ####\n",
    "#    model.load_state_dict(tc.load('/scratch/x2419a03/workspace/20220803/Cu_weights_100.pt'))\n",
    "#    model.desc.rcut = tc.tensor([rcut]).double().to(device=device)\n",
    "    ####\n",
    "    \n",
    "    imgdataset, dataloader = get_dataloader(location, symbol, number, 'training.json', batch_size=batch_size)\n",
    "    test_imgdataset, test_dataloader = get_dataloader(location, symbol, number, 'test.json', batch_size=batch_size)\n",
    "\n",
    "\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    normalizer = Normalizer(tc.tensor(imgdataset.energies).double(), len(imgdataset.symbols))\n",
    "\n",
    "    lr, muF = 1e-1, 3\n",
    "    for t in range(5000):\n",
    "        loss_fn = MSEFLoss(muF=muF)\n",
    "        lossE, lossG = train(dataloader, model, loss_fn, \n",
    "                             tc.optim.Adam(model.parameters(), lr=lr), normalizer)\n",
    "        if t % 10 == 0:\n",
    "            tc.save(model.state_dict(), './20220804/' + symbol + '_weights_%d.pt' % t)\n",
    "        loss = lossE + lossG\n",
    "        \n",
    "\n",
    "        # 30 meV\n",
    "        if loss < 3e-2:\n",
    "            lr = 1e-5\n",
    "            muF = 1\n",
    "        # 100 meV\n",
    "        elif loss < 1e-2:\n",
    "            lr = 1e-4\n",
    "            muF = 1.5\n",
    "        # 300 meV\n",
    "        elif loss < 3e-1:\n",
    "            lr = 1e-3\n",
    "            muF = 2\n",
    "        # 1 eV\n",
    "        elif loss < 1:\n",
    "            lr = 1e-2\n",
    "            muF = 2.5\n",
    "            \n",
    "#         # 30 meV\n",
    "#         if lossG < 3e-2:\n",
    "#             muF = 1\n",
    "#         # 100 meV\n",
    "#         if lossG < 1e-2:\n",
    "#             muF = 3\n",
    "#         # 300 meV\n",
    "#         elif lossG < 3e-2:\n",
    "#             muF = 10\n",
    "#         # 1 eV meV\n",
    "#         elif lossG < 1e-1:\n",
    "#             muF = 30\n",
    "\n",
    "        writer.add_scalar('training RMSE-E (eV/atom)', lossE, t)\n",
    "        writer.add_scalar('training RMSE-F (eV/A)', lossG, t)\n",
    "        \n",
    "        test_lossE, test_lossG = test(test_dataloader, model, loss_fn, normalizer)\n",
    "        writer.add_scalar('test RMSE-E (eV/atom)', test_lossE, t)\n",
    "        writer.add_scalar('test RMSE-F (eV/A)', test_lossG, t)\n",
    "        \n",
    "        print(t, \": {0:<10.2f} {1:<10.2f} {2:<10.2f} {3:<10.2f}\".format(lossE * 1000, lossG* 1000, test_lossE* 1000, test_lossG* 1000))\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    tc.cuda.empty_cache()\n",
    "    \n",
    "device = 'cuda'\n",
    "#tables = {'Cu': 29, 'Ge': 32, 'Li': 3, 'Mo': 42, 'Ni':28, 'Si': 14}\n",
    "tables = {'Cu': 29}\n",
    "\n",
    "\n",
    "from mlip.reann import Gj\n",
    "\n",
    "device='cuda'\n",
    "desc_NO = 4\n",
    "species = [0]\n",
    "nmax = 20\n",
    "loop = 3\n",
    "rcut = 3.\n",
    "\n",
    "moduledict = nn.ModuleDict()\n",
    "for spe in species:\n",
    "    moduledict[str(spe)] = nn.Sequential(\n",
    "        nn.Linear(desc_NO, 128),\n",
    "        nn.Softplus(),\n",
    "        nn.Linear(128, 1),\n",
    "    )\n",
    "    \n",
    "modulelist = nn.ModuleList()\n",
    "for j in range(loop):\n",
    "    descdict = nn.ModuleDict()\n",
    "    for spe in species:\n",
    "        descdict[str(spe)] = nn.Sequential(\n",
    "                nn.Linear(desc_NO, 128),\n",
    "                 nn.Softplus(),\n",
    "                nn.Linear(128, nmax)\n",
    "        )\n",
    "    modulelist.append(Gj(descdict, species=species, nmax=nmax))\n",
    "\n",
    "for sym, num in tables.items():\n",
    "    run(sym, num, device=device, nmax=nmax, loop=loop, rcut=rcut,\n",
    "        moduledict=moduledict.double().to(device=device),\n",
    "        modulelist=modulelist.double().to(device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello I am JaeHwan shim from jaejun Yu's group. \n",
    "\n",
    "I am going to explain about the Recursive embdding atomic nerual network. Recursive embedding neural network is neural network interatomic potential model which has foreground on the physics. \n",
    "\n",
    "Contents of my presentation is\n",
    "\n",
    "1. REANN 적용 현황\n",
    "\n",
    "single atoms\n",
    "\n",
    "\n",
    "\n",
    "Results \n",
    "\n",
    "Results and discussion \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Cu': 29, 'Ge': 32, 'Li': 3, 'Mo': 42, 'Ni':28, 'Si': 14"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple",
   "language": "python",
   "name": "simple"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
