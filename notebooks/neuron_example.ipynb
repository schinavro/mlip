{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/x2419a03/.conda/envs/simple/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from mlip.pes import PotentialNeuralNet\n",
    "from mlip.reann import REANN, compress_symbols\n",
    "\n",
    "def gen_module(species=None, nmax=2, lmax=10, loop=2, device='cpu'):\n",
    "\n",
    "    encode, decode, numbers = compress_symbols(species)\n",
    "    species = list(set(numbers))\n",
    "    reann = REANN(species, nmax=nmax, lmax=lmax, loop=loop, device=device)\n",
    "\n",
    "    moduledict = nn.ModuleDict()\n",
    "    desc = reann\n",
    "    for spe in species:\n",
    "        moduledict[str(spe)] = nn.Sequential(\n",
    "            nn.Linear(desc.NO, int(desc.NO*1.3)),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(int(desc.NO*1.3), 1)\n",
    "        )\n",
    "    moduledict = moduledict.double().to(device=device)\n",
    "\n",
    "    return PotentialNeuralNet(desc, moduledict, species)\n",
    "    \n",
    "model = gen_module(species=[29], lmax = 2, nmax = 15, loop = 2, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nomenclature\n",
    "# SPECG-CriP(symbols, positions, energies, cells, gradients, crystalindex, pbcs)\n",
    "\n",
    "import torch as tc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BPTypeDataset(Dataset):\n",
    "    \n",
    "    \"\"\"Behler Parrinello Type datasets\n",
    "    Indexing should be done in the unit of crystal, a set of atom used in one calculation. \n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        symbols: List\n",
    "        positions: List\n",
    "        energies: List\n",
    "        cells: List\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, symbols, positions, energies, cells, gradients, crystalidx, pbcs):\n",
    "        self.symbols = symbols\n",
    "        self.positions = positions\n",
    "        self.energies = energies\n",
    "        self.cells = cells\n",
    "        self.gradients = gradients\n",
    "        self.crystalidx = crystalidx\n",
    "        self.pbcs = pbcs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.energies)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.symbols[idx], self.positions[idx], self.energies[idx], self.cells[idx], self.gradients[idx], self.crystalidx[idx], self.pbcs[idx]\n",
    "\n",
    "    \n",
    "def concate(batch, device='cuda'):\n",
    "    cat = lambda x: tc.from_numpy(np.concatenate(x))\n",
    "    \n",
    "    symbols, positions, energies, cells, gradients, crystalidx, pbcs = [], [], [], [], [], [], []\n",
    "    for data in batch:\n",
    "        symbol, position, energy, cell, gradient, crystali, pbc = data\n",
    "        symbols.append(symbol)\n",
    "        positions.append(position)\n",
    "        energies.append(energy)\n",
    "        cells.append(cell[None])\n",
    "        gradients.append(gradient)\n",
    "        crystalidx.append(crystali)\n",
    "        pbcs.append(pbc[None])      \n",
    "\n",
    "    return (cat(symbols), cat(positions).to(device=device).requires_grad_(True), \n",
    "            tc.tensor(energies).to(device=device), cat(cells).to(device=device).requires_grad_(True), \n",
    "            cat(gradients).to(device=device), cat(crystalidx).to(device=device), cat(pbcs))\n",
    "\n",
    "import numpy as np\n",
    "from pymatgen.core import Structure\n",
    "from monty.serialization import loadfn\n",
    "\n",
    "def get_dataloader(location, symbol, number, name, batch_size=10):\n",
    "\n",
    "    data = loadfn(location + symbol + '/' + name)\n",
    "    encode, decode, numbers = compress_symbols([number])\n",
    "\n",
    "    #data[0]['structure'].cart_coords;\n",
    "    #data[0]['structure'].lattice.matrix;\n",
    "    #data[0]['outputs']['forces'];\n",
    "    #data[0]['structure'].lattice.pbc;\n",
    "    #data[0]['num_atoms']\n",
    "\n",
    "    symbols = [[encode[n] for n in d['structure'].atomic_numbers] for d in data]\n",
    "    positions = [d['structure'].cart_coords for d in data]\n",
    "    energies = [d['outputs']['energy'] for d in data]\n",
    "    cells = [d['structure'].lattice.matrix for d in data]\n",
    "    gradients = [-np.array(d['outputs']['forces']) for d in data]\n",
    "\n",
    "    crystalidx = [[idx] * data[idx]['num_atoms'] for idx in range(len(data))]\n",
    "    pbcs = [np.array(d['structure'].lattice.pbc) for d in data]\n",
    "\n",
    "    imgdataset = BPTypeDataset(symbols, positions, energies, cells, gradients, crystalidx, pbcs)\n",
    "    \n",
    "    return imgdataset, DataLoader(imgdataset, batch_size=batch_size, shuffle=True, collate_fn=concate)\n",
    "\n",
    "location = \"/home01/x2419a03/libCalc/mlip/data/\"\n",
    "symbol = 'Cu'\n",
    "number = 29\n",
    "imgdataset, dataloader = get_dataloader(location, symbol, number, 'test.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEFLoss:\n",
    "    def __init__(self, muE=1., muF=10.):\n",
    "        self.muE = muE\n",
    "        self.muF = muF\n",
    "    def __call__(self, predE, predF, y, dy):\n",
    "        self.lossE = tc.sum((y - predE) ** 2)\n",
    "        self.lossG = tc.sum((predF - dy)**2)\n",
    "        return self.muE * self.lossE + self.muF * self.lossG\n",
    "\n",
    "\n",
    "class Normalizer(object):\n",
    "    \"\"\"Normalize a Tensor and restore it later. \"\"\"\n",
    "\n",
    "    def __init__(self, tensor, device='cuda'):\n",
    "        \"\"\"tensor is taken as a sample to calculate the mean and std\"\"\"\n",
    "        self.mean = tc.mean(tensor).to(device=device)\n",
    "        self.std = tc.std(tensor).to(device=device)\n",
    "\n",
    "    def norm(self, tensor):\n",
    "        return (tensor - self.mean) / self.std\n",
    "\n",
    "    def denorm(self, normed_tensor):\n",
    "        return normed_tensor * self.std + self.mean\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {'mean': self.mean,\n",
    "                'std': self.std}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.mean = state_dict['mean']\n",
    "        self.std = state_dict['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as tc\n",
    "from torch.autograd import grad\n",
    "\n",
    "def test(dataloader, model, loss_fn, device='cuda'):\n",
    "    \n",
    "    lossE, lossG, NTA = 0., 0., 0\n",
    "    for batch, _ in enumerate(dataloader):\n",
    "\n",
    "        symbols, positions, energies, cells, gradients, crystalidx, pbcs = _\n",
    "\n",
    "        _, pred, predG = model(symbols, positions, cells, crystalidx, pbcs)\n",
    "        \n",
    "        loss = loss_fn(pred, predG, energies, gradients)\n",
    "        \n",
    "        A = len(gradients)\n",
    "        NTA += A\n",
    "        lossE += loss_fn.lossE.item()\n",
    "        lossG += loss_fn.lossG.item()\n",
    "\n",
    "    return lossE / NTA, lossG / NTA\n",
    "    \n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, normalizer, device='cuda'):\n",
    "    \n",
    "    lossE, lossG, NTA = 0., 0., 0\n",
    "    model.train()\n",
    "    for batch, _ in enumerate(dataloader):\n",
    "\n",
    "        symbols, positions, energies, cells, gradients, crystalidx, pbcs = _\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        _, pred, predG = model(symbols, positions, cells, crystalidx, pbcs)\n",
    "        \n",
    "        #loss = loss_fn(pred, predG, normalizer.norm(tc.tensor(energies)), gradients)\n",
    "        loss = loss_fn(pred, predG, energies, gradients)\n",
    "        \n",
    "        loss.requires_grad_(True)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        A = len(gradients)\n",
    "        NTA += A\n",
    "        lossE += loss_fn.lossE.item()\n",
    "        lossG += loss_fn.lossG.item()\n",
    "\n",
    "    return lossE / NTA, lossG / NTA\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def run(symbol, number, lmax=2, nmax=15, loop=2, batch_size=30, \n",
    "        location=\"/home01/x2419a03/libCalc/mlip/data/\", \n",
    "        device='cpu'):\n",
    "    species = [number]\n",
    "\n",
    "    log_dir = './20220727/' + symbol + '_log'\n",
    "    \n",
    "    model = gen_module(species=species, lmax=lmax, nmax=nmax, loop=loop, device=device)\n",
    "\n",
    "    imgdataset, dataloader = get_dataloader(location, symbol, number, 'training.json', batch_size=batch_size)\n",
    "    test_imgdataset, test_dataloader = get_dataloader(location, symbol, number, 'test.json', batch_size=batch_size)\n",
    "\n",
    "\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    normalizer = Normalizer(tc.tensor(imgdataset.energies).double())\n",
    "\n",
    "    for t in range(5000):\n",
    "\n",
    "        lossE, lossG = train(dataloader, model, MSEFLoss(), \n",
    "                             tc.optim.Adam(model.parameters(), lr=1e-2), normalizer)\n",
    "        if t % 10 == 0:\n",
    "            tc.save(model.state_dict(), './20220727/' + symbol + '_weights_%d.pt' % t)\n",
    "\n",
    "        writer.add_scalar('training RMSE-E (eV/atom)', lossE, t)\n",
    "        writer.add_scalar('training RMSE-F (eV/A)', lossG, t)\n",
    "        \n",
    "        test_lossE, test_lossG = test(test_dataloader, model, MSEFLoss())\n",
    "        writer.add_scalar('test RMSE-E (eV/atom)', test_lossE, t)\n",
    "        writer.add_scalar('test RMSE-F (eV/A)', test_lossG, t)\n",
    "        \n",
    "        print(t, \": {0:<10.3f} {1:<10.3f} {2:<10.3f} {3:<10.3f}\".format(lossE, lossG, test_lossE, test_lossG))\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    tc.cuda.empty_cache()\n",
    "    \n",
    "device = 'cuda'\n",
    "#tables = {'Cu': 29, 'Ge': 32, 'Li': 3, 'Mo': 42, 'Ni':28, 'Si': 14}\n",
    "tables = {'Cu': 29}\n",
    "\n",
    "for sym, num in tables.items():\n",
    "    run(sym, num, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple",
   "language": "python",
   "name": "simple"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
