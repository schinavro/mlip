{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9329c0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schinavro/anaconda3/envs/simple/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from taps.ml.descriptors.torch import REANN, compress_symbols\n",
    "\n",
    "_, decode, numbers = compress_symbols([64, 23, 1, 5, 5])\n",
    "species = list(set(numbers))\n",
    "reann = REANN(species)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "324d9b95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 268.4788,  311.9956,  323.2234, 2000.9879],\n",
       "        [ 503.2695,  481.0232,  571.4376, 4129.0348],\n",
       "        [ 514.0546,  614.2034,  624.1980, 3775.9683],\n",
       "        ...,\n",
       "        [ 231.8496,  257.8349,  275.3546, 1768.4474],\n",
       "        [ 203.8622,  253.4926,  250.6526, 1465.4909],\n",
       "        [ 164.4260,  154.8325,  185.9675, 1356.5326]], dtype=torch.float64,\n",
       "       grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch as tc\n",
    "\n",
    "NTA = 280\n",
    "NC = 10\n",
    "cutoff = 6.\n",
    "\n",
    "basis = tc.rand(NTA, 3).double().requires_grad_(True)\n",
    "\n",
    "numbers = tc.randint(4, (NTA,))\n",
    "lattices = (6*tc.eye(3)[None]+tc.zeros(NC, 1, 1 )).double().requires_grad_(True)\n",
    "pbcs = tc.ones(NC, 3, dtype=bool)\n",
    "crystalidx = tc.randint(NC, (NTA,))\n",
    "\n",
    "reann(numbers, basis, lattices, crystalidx, pbcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0862919",
   "metadata": {},
   "source": [
    "# Descriptor test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1998bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.0663, -0.0386, -0.0564, -0.0639],\n",
      "        [-0.1013, -0.0571, -0.0804, -0.0969],\n",
      "        [ 0.1718,  0.0998,  0.1452,  0.1655],\n",
      "        [-0.2007, -0.1132, -0.1596, -0.1921]], dtype=torch.float64,\n",
      "       grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch as tc\n",
    "import numpy as np\n",
    "from numpy import sin, cos\n",
    "from torch import nn\n",
    "from taps.projectors.tensor import Tensor\n",
    "#from taps.projectors.projector import Mask2\n",
    "from taps.ml.descriptors.torch import REANN\n",
    "app\n",
    "r, rr, rrr = 1, 1, 1\n",
    "q, α, θ, ϕ = 1, 0.2, 2., 3.1\n",
    "\n",
    "ra = r * np.array([0, cos(q), sin(q)])\n",
    "rb = r * np.array([0, -cos(q), sin(q)])\n",
    "r1 = rr * np.array([cos(α), 0, sin(α)])\n",
    "r2 = rrr * np.array([sin(θ)*cos(ϕ), sin(θ)*sin(ϕ), cos(θ)])\n",
    "r3 = rrr * np.array([-sin(θ)*cos(ϕ), -sin(θ)*sin(ϕ), cos(θ)])\n",
    "\n",
    "positions1 = np.array([np.zeros(3), ra, r1, r2, r3])\n",
    "positions2 = np.array([np.zeros(3), rb, r1, r2, r3])\n",
    "\n",
    "pbc = tc.Tensor([False, False, False]).bool()\n",
    "cell = tc.eye(3).double() * 10\n",
    "\n",
    "positions = positions1\n",
    "#coords = np.ones(100)[:, None, None] * positions\n",
    "coords = positions\n",
    "encode = {6:0, 1:1}\n",
    "numbers = tc.tensor([encode[n] for n in [6, 1, 1, 1, 1]]).long()\n",
    "crystalidx = tc.tensor([0] * 5).long()\n",
    "cutoffs = [6.]* len(numbers)\n",
    "\n",
    "desc = REANN(species=[0, 1], lmax=2, nmax=2, loop=1)\n",
    "# NxAxG\n",
    "#desc = Naive()\n",
    "#descriptor1 = desc(tc.from_numpy(co).double())\n",
    "descriptor1 = desc(numbers, tc.from_numpy(positions1).double().requires_grad_(True), \n",
    "                   cell[None], crystalidx, pbc)\n",
    "descriptor2 = desc(numbers, tc.from_numpy(positions2).double().requires_grad_(True),\n",
    "                  cell[None], crystalidx, pbc)\n",
    "\n",
    "print(descriptor1 - descriptor2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd21273c",
   "metadata": {},
   "outputs": [],
   "source": [
    "positions = tc.rand(NTA, 3).double().requires_grad_(True)\n",
    "numbers = tc.randint(4, (NTA,))\n",
    "cells = (6*tc.eye(3)[None]+tc.zeros(NC, 1, 1 )).double().requires_grad_(True)\n",
    "pbcs = tc.ones(NC, 3, dtype=bool)\n",
    "crystalidx = tc.randint(NC, (NTA,))\n",
    "cutoff = 6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b0612e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.autograd import grad\n",
    "device = 'cpu'\n",
    "\n",
    "class PotentialNeuralNet(nn.Module):\n",
    "    \"\"\" Behler-Parrinello type interatomic potential energy module\n",
    "    \n",
    "    $$ E(x) = \\sum_{i}^N E_i(x) $$\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        desc: torch.nn.Module class\n",
    "          Chemical descriptor sends cartesian tensor Tensor{Double} to descriptive tensor Tensor{Double}\n",
    "        moduledict: torch.nn.ModuleDict\n",
    "          For each species, \n",
    "        species: List of int\n",
    "    \n",
    "    Example\n",
    "    -------\n",
    "        >>> species = [0, 0, 0, 1]  # Compressed expression\n",
    "        >>> desc = REANN(...)\n",
    "        >>> moduledict = nn.ModuleDict()\n",
    "        >>> desc = reann\n",
    "        >>> for spe in species:\n",
    "        ...     moduledict[str(spe)] = nn.Sequential(\n",
    "        ...        nn.Linear(desc.NO, int(desc.NO*1.3)),\n",
    "        ...        nn.SiLU(),\n",
    "        ...        nn.Linear(int(desc.NO*1.3), 1)\n",
    "        ...    )\n",
    "        >>> moduledict = moduledict.double().to(device=device)\n",
    "        >>> model = PotentialNeuralNet(desc, moduledict, species)\n",
    "    \"\"\"\n",
    "    def __init__(self, desc, moduledict, species, **kwargs):\n",
    "        super(PotentialNeuralNet, self).__init__()\n",
    "        \n",
    "#        self.desc = desc\n",
    "        super(PotentialNeuralNet, self).add_module('desc', desc)\n",
    "        self.moduledict = moduledict\n",
    "        self.species = species       \n",
    "        \n",
    "    def forward(self, symbols, positions, cells, crystalidx, pbcs, cutoff=6.) -> tuple:\n",
    "        \"\"\" \n",
    "    \n",
    "        Parameters\n",
    "        ----------\n",
    "            numbers: NTA Tensor{Int} \n",
    "              periodic number of atomsㅛㅐㅕ\n",
    "            positions: NTA x 3 Tensor{Double} \n",
    "              Atomic positions\n",
    "            cells: NC x 3 x 3 Tensor{Double} \n",
    "              Lattice vector of `NC` number of atoms\n",
    "            crystalidx: NTA Tensor{Int}\n",
    "            pbcs: NC x 3 Tensor{Bool}\n",
    "        \n",
    "        Return\n",
    "        ------\n",
    "        Tuple of three tensors\n",
    "        (energies, energy, forces)\n",
    "        \n",
    "            energies: NTA Tensor{Double}\n",
    "            energy: NC Tensor{Double}\n",
    "            forces: NCx3 Tensor{Double}\n",
    "\n",
    "        \"\"\"\n",
    "        # Descriptor calculation\n",
    "        # NTA x 3 -> NTA x D\n",
    "        desc = self.desc(symbols, positions, cells, crystalidx, pbcs)\n",
    "        \n",
    "        positionsidx = tc.arange(len(positions))\n",
    "        energies, new_positionsidx = [], []\n",
    "        for spe in self.species:\n",
    "            # NTA -> NAS\n",
    "            smask = spe == symbols        \n",
    "            # NAS x NO -> NAS\n",
    "            energies.append(self.moduledict[str(spe)](desc[smask]))\n",
    "\n",
    "            new_positionsidx.append(positionsidx[smask])\n",
    "            \n",
    "        energies = tc.cat(energies)\n",
    "        new_positionsidx = tc.cat(new_positionsidx)\n",
    "        _, srtidx = tc.sort(new_positionsidx)\n",
    "        energies = energies[srtidx]\n",
    "        \n",
    "\n",
    "        energy, forces = [], []\n",
    "        for i, cry in enumerate(tc.unique(crystalidx)):\n",
    "            cmask = cry == crystalidx\n",
    "            V = tc.sum(energies[cmask])\n",
    "            \n",
    "            energy.append(V[None])            \n",
    "        energy = tc.cat(energy)\n",
    "        forces = grad(tc.sum(energy), positions, create_graph=True, allow_unused=True)[0]\n",
    "        \n",
    "        return energies[:, 0], energy, forces\n",
    "\n",
    "\n",
    "\n",
    "moduledict = nn.ModuleDict()\n",
    "desc = reann\n",
    "for spe in species:\n",
    "    moduledict[str(spe)] = nn.Sequential(\n",
    "        nn.Linear(desc.NO, int(desc.NO*1.3)),\n",
    "        nn.SiLU(),\n",
    "        nn.Linear(int(desc.NO*1.3), 1)\n",
    "    )\n",
    "moduledict = moduledict.double().to(device=device)\n",
    "    \n",
    "model = PotentialNeuralNet(desc, moduledict, species)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "38380706",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([280]), torch.Size([10]), torch.Size([280, 3]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "energes, energy, forces = model(numbers, positions, cells, crystalidx, pbcs)\n",
    "energes.shape, energy.shape, forces.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9376c615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nomenclature\n",
    "# SPECG-CriP(symbols, positions, energies, cells, gradients, crystalindex, pbcs)\n",
    "\n",
    "import torch as tc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BPTypeDataset(Dataset):\n",
    "    \n",
    "    \"\"\"Behler Parrinello Type datasets\n",
    "    Indexing should be done in the unit of crystal, a set of atom used in one calculation. \n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        symbols: List\n",
    "        positions: List\n",
    "        energies: List\n",
    "        cells: List\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, symbols, positions, energies, cells, gradients, crystalidx, pbcs):\n",
    "        self.symbols = symbols\n",
    "        self.positions = positions\n",
    "        self.energies = energies\n",
    "        self.cells = cells\n",
    "        self.gradients = gradients\n",
    "        self.crystalidx = crystalidx\n",
    "        self.pbcs = pbcs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.energies)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.symbols[idx], self.positions[idx], self.energies[idx], self.cells[idx], self.gradients[idx], self.crystalidx[idx], self.pbcs[idx]\n",
    "\n",
    "    \n",
    "def concate(batch, device='cpu'):\n",
    "    cat = lambda x: tc.from_numpy(np.concatenate(x))\n",
    "    \n",
    "    symbols, positions, energies, cells, gradients, crystalidx, pbcs = [], [], [], [], [], [], []\n",
    "    for data in batch:\n",
    "        symbol, position, energy, cell, gradient, crystali, pbc = data\n",
    "        symbols.append(symbol)\n",
    "        positions.append(position)\n",
    "        energies.append(energy)\n",
    "        cells.append(cell[None])\n",
    "        gradients.append(gradient)\n",
    "        crystalidx.append(crystali)\n",
    "        pbcs.append(pbc[None])      \n",
    "\n",
    "    return (cat(symbols), cat(positions).to(device=device).requires_grad_(True), \n",
    "            energies, cat(cells).to(device=device).requires_grad_(True), \n",
    "            cat(gradients), cat(crystalidx), cat(pbcs))\n",
    "\n",
    "\n",
    "from taps.ml.descriptors.torch import REANN, compress_symbols\n",
    "\n",
    "encode, decode, numbers = compress_symbols([29])\n",
    "species = list(set(numbers))\n",
    "reann = REANN(species)\n",
    "\n",
    "moduledict = nn.ModuleDict()\n",
    "desc = reann\n",
    "for spe in species:\n",
    "    moduledict[str(spe)] = nn.Sequential(\n",
    "        nn.Linear(desc.NO, int(desc.NO*1.3)),\n",
    "        nn.SiLU(),\n",
    "        nn.Linear(int(desc.NO*1.3), 1)\n",
    "    )\n",
    "moduledict = moduledict.double().to(device=device)\n",
    "    \n",
    "model = PotentialNeuralNet(desc, moduledict, species)\n",
    "\n",
    "from ase.io import read\n",
    "atoms_list = read('20220709/boltzmann.traj', index=':')\n",
    "import numpy as np\n",
    "#symbols = [a.symbols.numbers for a in atoms_list]\n",
    "symbols = [[encode[n] for n in a.symbols.numbers] for a in atoms_list]\n",
    "positions = [a.positions for a in atoms_list]\n",
    "energies = [a.calc.results['energy'] for a in atoms_list]\n",
    "cells = [a.cell.array for a in atoms_list]\n",
    "gradients = [-a.calc.results['forces'] for a in atoms_list]\n",
    "\n",
    "crystalidx = [[idx] * len(atoms_list[idx]) for idx in range(len(atoms_list))]\n",
    "pbcs = [a.pbc for a in atoms_list]\n",
    "           \n",
    "imgdataset = BPTypeDataset(symbols, positions, energies, cells, gradients, crystalidx, pbcs)\n",
    "dataloader = DataLoader(imgdataset, batch_size=100, shuffle=True, collate_fn=concate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b90e8d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEFLoss:\n",
    "    def __call__(self, predE, predF, y, dy):\n",
    "        N = len(y)\n",
    "        A = len(dy)\n",
    "        return tc.sum((y - predE) ** 2) / N +  tc.sum((predF - dy)**2) / A\n",
    "        \n",
    "    \n",
    "class Normalizer(object):\n",
    "    \"\"\"Normalize a Tensor and restore it later. \"\"\"\n",
    "\n",
    "    def __init__(self, tensor, device='cpu'):\n",
    "        \"\"\"tensor is taken as a sample to calculate the mean and std\"\"\"\n",
    "        self.mean = tc.mean(tensor).to(device=device)\n",
    "        self.std = tc.std(tensor).to(device=device)\n",
    "\n",
    "    def norm(self, tensor):\n",
    "        return (tensor - self.mean) / self.std\n",
    "\n",
    "    def denorm(self, normed_tensor):\n",
    "        return normed_tensor * self.std + self.mean\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {'mean': self.mean,\n",
    "                'std': self.std}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.mean = state_dict['mean']\n",
    "        self.std = state_dict['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f09046c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1780.0055, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1480.8781, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1480.8781, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1184.3322, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(907.2291, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(907.2291, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(636.7381, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(505.9329, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(505.9329, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(194.2113, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(33.9225, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(33.9225, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2.6731, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(48.8764, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(48.8764, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0198, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(41.7824, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(41.7824, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.2548, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(38.6963, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(38.6963, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(9.0249, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(32.0453, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(32.0453, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.7831, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(39.1244, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(39.1244, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(9.4865, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(43.3790, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(43.3790, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4.5807, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(41.3905, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(41.3905, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(8.0091, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(35.8464, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(35.8464, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 29>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m normalizer \u001b[38;5;241m=\u001b[39m Normalizer(tc\u001b[38;5;241m.\u001b[39mtensor(imgdataset\u001b[38;5;241m.\u001b[39menergies)\u001b[38;5;241m.\u001b[39mdouble())\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5000\u001b[39m):\n\u001b[0;32m---> 30\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMSEFLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                 \u001b[49m\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m     writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss/test\u001b[39m\u001b[38;5;124m'\u001b[39m, loss, t)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m t \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer, normalizer, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     10\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 12\u001b[0m _, pred, predG \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcells\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrystalidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbcs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, predG, normalizer\u001b[38;5;241m.\u001b[39mnorm(tc\u001b[38;5;241m.\u001b[39mtensor(energies)), gradients)\n\u001b[1;32m     15\u001b[0m loss\u001b[38;5;241m.\u001b[39mrequires_grad_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/simple/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36mPotentialNeuralNet.forward\u001b[0;34m(self, symbols, positions, cells, crystalidx, pbcs, cutoff)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     65\u001b[0m \n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;66;03m# Descriptor calculation\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# NTA x 3 -> NTA x D\u001b[39;00m\n\u001b[0;32m---> 69\u001b[0m desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcells\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrystalidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbcs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m positionsidx \u001b[38;5;241m=\u001b[39m tc\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(positions))\n\u001b[1;32m     73\u001b[0m energies, new_positionsidx \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/simple/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/simple/lib/python3.8/site-packages/taps/ml/descriptors/torch.py:364\u001b[0m, in \u001b[0;36mREANN.forward\u001b[0;34m(self, symbols, positions, cells, crystalidx, pbcs)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;124;03m----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    359\u001b[0m \u001b[38;5;124;03m    density ρ\u001b[39;00m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;66;03m# Number of crystals, Number of total atoms\u001b[39;00m\n\u001b[1;32m    363\u001b[0m iidx, jidx, isym, jsym, disp, dist \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 364\u001b[0m     \u001b[43mget_neighbors_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcells\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrystalidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbcs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m NTA \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(positions)\n\u001b[1;32m    367\u001b[0m dtype, device \u001b[38;5;241m=\u001b[39m positions\u001b[38;5;241m.\u001b[39mdtype, positions\u001b[38;5;241m.\u001b[39mdevice\n",
      "File \u001b[0;32m~/anaconda3/envs/simple/lib/python3.8/site-packages/taps/ml/descriptors/torch.py:240\u001b[0m, in \u001b[0;36mget_neighbors_info\u001b[0;34m(symbols, positions, cells, crystalidx, pbcs, cutoff)\u001b[0m\n\u001b[1;32m    238\u001b[0m pbc, cell \u001b[38;5;241m=\u001b[39m pbcs[c], cells[c]\n\u001b[1;32m    239\u001b[0m \u001b[38;5;66;03m# NN, NN, NNx3, NN\u001b[39;00m\n\u001b[0;32m--> 240\u001b[0m idx, jdx, isy, jsy, dsp, dst \u001b[38;5;241m=\u001b[39m \u001b[43mget_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    241\u001b[0m iidx\u001b[38;5;241m.\u001b[39mappend(crystali[idx])\n\u001b[1;32m    242\u001b[0m \u001b[38;5;66;03m# iidx.append(idx)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/simple/lib/python3.8/site-packages/taps/ml/descriptors/torch.py:173\u001b[0m, in \u001b[0;36mget_nn\u001b[0;34m(symbols, positions, cell, pbc, cutoff, device)\u001b[0m\n\u001b[1;32m    171\u001b[0m disp2 \u001b[38;5;241m=\u001b[39m kpositions[:, \u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m-\u001b[39m positions[\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# AxA\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m dist1 \u001b[38;5;241m=\u001b[39m \u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisp1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m dist2 \u001b[38;5;241m=\u001b[39m tc\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(disp2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# AxA\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch.autograd import grad\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, normalizer, device='cpu'):\n",
    "    model.train()\n",
    "    for batch, _ in enumerate(dataloader):\n",
    "\n",
    "        symbols, positions, energies, cells, gradients, crystalidx, pbcs = _\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        _, pred, predG = model(symbols, positions, cells, crystalidx, pbcs)\n",
    "        \n",
    "        lossE, lossG = loss_fn(pred, predG, normalizer.norm(tc.tensor(energies)), gradients)\n",
    "        loss = lossE + lossG\n",
    "        loss.requires_grad_(True)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss)\n",
    "\n",
    "    return lossE, lossG\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "writer = SummaryWriter(log_dir='./20220709/copper_log3')\n",
    "\n",
    "normalizer = Normalizer(tc.tensor(imgdataset.energies).double())\n",
    "\n",
    "for t in range(5000):\n",
    "    lossE, lossG = train(dataloader, model, MSEFLoss(), \n",
    "                         tc.optim.Adam(model.parameters(), lr=1e-4), normalizer)\n",
    "\n",
    "    writer.add_scalar('Loss / MSE energy (eV)', lossE, t)\n",
    "    writer.add_scalar('Loss / MSE grad (eV/A)', lossG, t)\n",
    "    if t % 10 == 0:\n",
    "        tc.save(model.state_dict(), '20220709/weights4_%d.pt' % t)\n",
    "    print(lossE + lossG)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126bc4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple",
   "language": "python",
   "name": "simple"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
