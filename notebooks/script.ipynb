{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40a76157",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schinavro/anaconda3/envs/simple/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from mlip.pes import PotentialNeuralNet\n",
    "from mlip.reann import REANN, compress_symbols\n",
    "\n",
    "species = [29]\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "lmax = 2\n",
    "nmax = 15\n",
    "loop = 2\n",
    "\n",
    "encode, decode, numbers = compress_symbols(species)\n",
    "species = list(set(numbers))\n",
    "reann = REANN(species, nmax=nmax, lmax=lmax, loop=loop)\n",
    "\n",
    "moduledict = nn.ModuleDict()\n",
    "desc = reann\n",
    "for spe in species:\n",
    "    moduledict[str(spe)] = nn.Sequential(\n",
    "        nn.Linear(desc.NO, int(desc.NO*1.3)),\n",
    "        nn.SiLU(),\n",
    "        nn.Linear(int(desc.NO*1.3), 1)\n",
    "    )\n",
    "moduledict = moduledict.double().to(device=device)\n",
    "    \n",
    "model = PotentialNeuralNet(desc, moduledict, species)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92ca1be",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ccf6f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pymatgen.core import Structure\n",
    "from monty.serialization import loadfn\n",
    "location = \"../data/Cu/\"\n",
    "data = loadfn(location + 'training.json')\n",
    "\n",
    "#data[0]['structure'].cart_coords;\n",
    "#data[0]['structure'].lattice.matrix;\n",
    "#data[0]['outputs']['forces'];\n",
    "#data[0]['structure'].lattice.pbc;\n",
    "#data[0]['num_atoms']\n",
    "\n",
    "symbols = [[encode[n] for n in d['structure'].atomic_numbers] for d in data]\n",
    "positions = [d['structure'].cart_coords for d in data]\n",
    "energies = [d['outputs']['energy'] for d in data]\n",
    "cells = [d['structure'].lattice.matrix for d in data]\n",
    "gradients = [-np.array(d['outputs']['forces']) for d in data]\n",
    "\n",
    "crystalidx = [[idx] * data[idx]['num_atoms'] for idx in range(len(data))]\n",
    "pbcs = [np.array(d['structure'].lattice.pbc) for d in data]\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e799e9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nomenclature\n",
    "# SPECG-CriP(symbols, positions, energies, cells, gradients, crystalindex, pbcs)\n",
    "\n",
    "import torch as tc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BPTypeDataset(Dataset):\n",
    "    \n",
    "    \"\"\"Behler Parrinello Type datasets\n",
    "    Indexing should be done in the unit of crystal, a set of atom used in one calculation. \n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        symbols: List\n",
    "        positions: List\n",
    "        energies: List\n",
    "        cells: List\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, symbols, positions, energies, cells, gradients, crystalidx, pbcs):\n",
    "        self.symbols = symbols\n",
    "        self.positions = positions\n",
    "        self.energies = energies\n",
    "        self.cells = cells\n",
    "        self.gradients = gradients\n",
    "        self.crystalidx = crystalidx\n",
    "        self.pbcs = pbcs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.energies)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.symbols[idx], self.positions[idx], self.energies[idx], self.cells[idx], self.gradients[idx], self.crystalidx[idx], self.pbcs[idx]\n",
    "\n",
    "    \n",
    "def concate(batch, device='cpu'):\n",
    "    cat = lambda x: tc.from_numpy(np.concatenate(x))\n",
    "    \n",
    "    symbols, positions, energies, cells, gradients, crystalidx, pbcs = [], [], [], [], [], [], []\n",
    "    for data in batch:\n",
    "        symbol, position, energy, cell, gradient, crystali, pbc = data\n",
    "        symbols.append(symbol)\n",
    "        positions.append(position)\n",
    "        energies.append(energy)\n",
    "        cells.append(cell[None])\n",
    "        gradients.append(gradient)\n",
    "        crystalidx.append(crystali)\n",
    "        pbcs.append(pbc[None])      \n",
    "\n",
    "    return (cat(symbols), cat(positions).to(device=device).requires_grad_(True), \n",
    "            energies, cat(cells).to(device=device).requires_grad_(True), \n",
    "            cat(gradients), cat(crystalidx), cat(pbcs))\n",
    "\n",
    "imgdataset = BPTypeDataset(symbols, positions, energies, cells, gradients, crystalidx, pbcs)\n",
    "dataloader = DataLoader(imgdataset, batch_size=10, shuffle=True, collate_fn=concate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa73f931",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEFLoss:\n",
    "    def __call__(self, predE, predF, y, dy):\n",
    "        N = len(y)\n",
    "        A = len(dy)\n",
    "        self.lossE = tc.sum((y - predE) ** 2) / N\n",
    "        self.lossG = tc.sum((predF - dy)**2) / A\n",
    "        return self.lossE + self.lossG\n",
    "\n",
    "\n",
    "class Normalizer(object):\n",
    "    \"\"\"Normalize a Tensor and restore it later. \"\"\"\n",
    "\n",
    "    def __init__(self, tensor, device='cpu'):\n",
    "        \"\"\"tensor is taken as a sample to calculate the mean and std\"\"\"\n",
    "        self.mean = tc.mean(tensor).to(device=device)\n",
    "        self.std = tc.std(tensor).to(device=device)\n",
    "\n",
    "    def norm(self, tensor):\n",
    "        return (tensor - self.mean) / self.std\n",
    "\n",
    "    def denorm(self, normed_tensor):\n",
    "        return normed_tensor * self.std + self.mean\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {'mean': self.mean,\n",
    "                'std': self.std}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.mean = state_dict['mean']\n",
    "        self.std = state_dict['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6cd227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(827949.6567, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(47408.4957, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(16957.3239, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4.2451e+15, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(42683.7181, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(100323.4514, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3.3221e+15, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.0363e+08, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.1238e+09, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.3722e+11, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(212449.2332, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(234602.6862, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.1062e+15, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(202908.3980, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(199802.7297, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(187960.6382, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(175114.3894, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.8464e+13, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(180372.0255, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3.5125e+08, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(171654.6827, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(174060.2940, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(170929.2796, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(172929.0230, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2.7070e+09, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(175777.6421, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(172516.0985, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(163191.0446, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(165423.9001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(151556.0261, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(127975.8674, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(93536.2335, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(48168.1800, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(113619.5234, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(19253.9724, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(9.4216e+10, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.2023e+13, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(57219.2247, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0651e+16, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(87118.1272, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(106576.5103, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(106247.2412, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2.2965e+09, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(135412.4122, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(135885.2972, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(133426.9944, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.6720e+15, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(135280.8355, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.8518e+11, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9596e+15, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(138572.4732, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(140416.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(136698.8269, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(131247.9162, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(149179.0649, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5817e+09, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(117864.2656, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(114439.3978, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(12903128.0789, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3.7725e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(106850.2806, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(118378.8220, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(121151.3950, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(129574.2320, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(123731.6401, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(118296.4931, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.1290e+11, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(131685.2561, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(137371.8973, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(134688.2902, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(138558.9001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(139824.0196, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(139342.0882, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(142958.9346, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(133760.3962, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(141378.2830, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(152013.3664, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(162570.3947, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(147503.1389, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(468220.9321, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(136594.4264, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(150564.9784, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4499915.5239, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(137937.2878, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.0042e+11, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(159286.8288, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(139521.5843, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(135342.1380, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(131969.2037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(126162.9633, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(125763.8444, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(122323.6919, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(436805.5263, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(117281.4972, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(443377.6908, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(115272.0568, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(110307.8835, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(116738.1635, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(115082.8391, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2672806.5881, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(106084.7869, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(111794.6464, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(107223.2876, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(100400.4279, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(13467647.4940, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(97737.5211, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(99658.3088, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(114322.0069, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(99772.2075, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(285808.6069, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(82477.4311, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(94062.8698, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(102452.2804, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(98177.6029, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(330345.0509, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(112670.2739, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(124169.5300, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(127839.2884, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(174439.2166, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(139922.7992, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4818008.6603, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(176629.1908, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(111331.3725, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(103371.4147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(100475.4826, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(97598.3692, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(100683.9035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(97977.8154, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(101455.2819, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(95488.0992, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(204133.3647, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(97390.1728, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(78519.3005, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch as tc\n",
    "from torch.autograd import grad\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, normalizer, device='cpu'):\n",
    "    model.train()\n",
    "    for batch, _ in enumerate(dataloader):\n",
    "\n",
    "        symbols, positions, energies, cells, gradients, crystalidx, pbcs = _\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        _, pred, predG = model(symbols, positions, cells, crystalidx, pbcs)\n",
    "        \n",
    "        #loss = loss_fn(pred, predG, normalizer.norm(tc.tensor(energies)), gradients)\n",
    "        loss = loss_fn(pred, predG, tc.tensor(energies), gradients)\n",
    "        \n",
    "        loss.requires_grad_(True)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss_fn.lossE.data, loss_fn.lossG.data)\n",
    "\n",
    "    return loss_fn.lossE, loss_fn.lossG\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir='./20220727/copper_log')\n",
    "\n",
    "normalizer = Normalizer(tc.tensor(imgdataset.energies).double())\n",
    "\n",
    "for t in range(5000):\n",
    "    lossE, lossG = train(dataloader, model, MSEFLoss(), \n",
    "                         tc.optim.Adam(model.parameters(), lr=1e-2), normalizer)\n",
    "\n",
    "    writer.add_scalar('Loss / MSE energy (eV)', lossE, t)\n",
    "    writer.add_scalar('Loss / MSE grad (eV/A)', lossG, t)\n",
    "    if t % 10 == 0:\n",
    "        tc.save(model.state_dict(), './20220727/weights_%d.pt' % t)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df97905",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple",
   "language": "python",
   "name": "simple"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
