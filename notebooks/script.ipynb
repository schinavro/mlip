{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9357b05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schinavro/anaconda3/envs/simple/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from mlip.pes import PotentialNeuralNet\n",
    "from mlip.reann import REANN, compress_symbols\n",
    "\n",
    "species = [29]\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "lmax = 2\n",
    "nmax = 10\n",
    "loop = 3\n",
    "\n",
    "encode, decode, numbers = compress_symbols(species)\n",
    "species = list(set(numbers))\n",
    "reann = REANN(species, nmax=nmax, lmax=lmax, loop=loop)\n",
    "\n",
    "moduledict = nn.ModuleDict()\n",
    "desc = reann\n",
    "for spe in species:\n",
    "    moduledict[str(spe)] = nn.Sequential(\n",
    "        nn.Linear(desc.NO, int(desc.NO*1.3)),\n",
    "        nn.SiLU(),\n",
    "        nn.Linear(int(desc.NO*1.3), 1)\n",
    "    )\n",
    "moduledict = moduledict.double().to(device=device)\n",
    "    \n",
    "model = PotentialNeuralNet(desc, moduledict, species)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3cebde8",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bd925eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pymatgen.core import Structure\n",
    "from monty.serialization import loadfn\n",
    "location = \"../data/Cu/\"\n",
    "data = loadfn(location + 'training.json')\n",
    "\n",
    "#data[0]['structure'].cart_coords;\n",
    "#data[0]['structure'].lattice.matrix;\n",
    "#data[0]['outputs']['forces'];\n",
    "#data[0]['structure'].lattice.pbc;\n",
    "#data[0]['num_atoms']\n",
    "\n",
    "symbols = [[encode[n] for n in d['structure'].atomic_numbers] for d in data]\n",
    "positions = [d['structure'].cart_coords for d in data]\n",
    "energies = [d['outputs']['energy'] for d in data]\n",
    "cells = [d['structure'].lattice.matrix for d in data]\n",
    "gradients = [-np.array(d['outputs']['forces']) for d in data]\n",
    "\n",
    "crystalidx = [[idx] * data[idx]['num_atoms'] for idx in range(len(data))]\n",
    "pbcs = [np.array(d['structure'].lattice.pbc) for d in data]\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5f5608a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nomenclature\n",
    "# SPECG-CriP(symbols, positions, energies, cells, gradients, crystalindex, pbcs)\n",
    "\n",
    "import torch as tc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BPTypeDataset(Dataset):\n",
    "    \n",
    "    \"\"\"Behler Parrinello Type datasets\n",
    "    Indexing should be done in the unit of crystal, a set of atom used in one calculation. \n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        symbols: List\n",
    "        positions: List\n",
    "        energies: List\n",
    "        cells: List\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, symbols, positions, energies, cells, gradients, crystalidx, pbcs):\n",
    "        self.symbols = symbols\n",
    "        self.positions = positions\n",
    "        self.energies = energies\n",
    "        self.cells = cells\n",
    "        self.gradients = gradients\n",
    "        self.crystalidx = crystalidx\n",
    "        self.pbcs = pbcs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.energies)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.symbols[idx], self.positions[idx], self.energies[idx], self.cells[idx], self.gradients[idx], self.crystalidx[idx], self.pbcs[idx]\n",
    "\n",
    "    \n",
    "def concate(batch, device='cpu'):\n",
    "    cat = lambda x: tc.from_numpy(np.concatenate(x))\n",
    "    \n",
    "    symbols, positions, energies, cells, gradients, crystalidx, pbcs = [], [], [], [], [], [], []\n",
    "    for data in batch:\n",
    "        symbol, position, energy, cell, gradient, crystali, pbc = data\n",
    "        symbols.append(symbol)\n",
    "        positions.append(position)\n",
    "        energies.append(energy)\n",
    "        cells.append(cell[None])\n",
    "        gradients.append(gradient)\n",
    "        crystalidx.append(crystali)\n",
    "        pbcs.append(pbc[None])      \n",
    "\n",
    "    return (cat(symbols), cat(positions).to(device=device).requires_grad_(True), \n",
    "            energies, cat(cells).to(device=device).requires_grad_(True), \n",
    "            cat(gradients), cat(crystalidx), cat(pbcs))\n",
    "\n",
    "imgdataset = BPTypeDataset(symbols, positions, energies, cells, gradients, crystalidx, pbcs)\n",
    "dataloader = DataLoader(imgdataset, batch_size=10, shuffle=True, collate_fn=concate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac12941c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEFLoss:\n",
    "    def __call__(self, predE, predF, y, dy):\n",
    "        N = len(y)\n",
    "        A = len(dy)\n",
    "        self.lossE = tc.sum((y - predE) ** 2) / N\n",
    "        self.lossG = tc.sum((predF - dy)**2) / A\n",
    "        return self.lossE + self.lossG\n",
    "\n",
    "\n",
    "class Normalizer(object):\n",
    "    \"\"\"Normalize a Tensor and restore it later. \"\"\"\n",
    "\n",
    "    def __init__(self, tensor, device='cpu'):\n",
    "        \"\"\"tensor is taken as a sample to calculate the mean and std\"\"\"\n",
    "        self.mean = tc.mean(tensor).to(device=device)\n",
    "        self.std = tc.std(tensor).to(device=device)\n",
    "\n",
    "    def norm(self, tensor):\n",
    "        return (tensor - self.mean) / self.std\n",
    "\n",
    "    def denorm(self, normed_tensor):\n",
    "        return normed_tensor * self.std + self.mean\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {'mean': self.mean,\n",
    "                'std': self.std}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.mean = state_dict['mean']\n",
    "        self.std = state_dict['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26a528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ -131.5893,  -185.0634,  -609.3994,  2372.9529, -3006.9062,  4890.5400,\n",
      "         -1542.9200,  -483.6465,  5620.0674,   235.9423]])\n",
      "tensor(6474.4111, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([[  29.0423, -455.2028, -229.9046,  432.2812, -962.9335, 1033.6934,\n",
      "         -309.9249, -201.7359,  180.8158,  121.3441]])\n",
      "tensor(3551.4508, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([[  -16.4997,  -463.4565,  -145.4250,   193.0038,  -546.4291,   387.7461,\n",
      "          -744.3372,  -156.9509, -1191.2640,    89.7487]])\n",
      "tensor(2693.5206, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([[  67.9666, -750.5685,    1.0126,   53.3926, -304.9118,    6.0345,\n",
      "          577.2721,  -29.3815, -368.6872,   44.7145]])\n",
      "tensor(2077.0459, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor([[ 8.6188e+18, -1.0924e+19,  1.9780e+18,  1.1382e+17, -5.1406e+17,\n",
      "          4.7780e+18,  5.1057e+19,  4.7258e+18,  3.0867e+19,  2.4055e+18]])\n",
      "tensor(2.8715e+18, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch as tc\n",
    "from torch.autograd import grad\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, normalizer, device='cpu'):\n",
    "    model.train()\n",
    "    for batch, _ in enumerate(dataloader):\n",
    "\n",
    "        symbols, positions, energies, cells, gradients, crystalidx, pbcs = _\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        _, pred, predG = model(symbols, positions, cells, crystalidx, pbcs)\n",
    "        \n",
    "        loss = loss_fn(pred, predG, normalizer.norm(tc.tensor(energies)), gradients)\n",
    "        \n",
    "        loss.requires_grad_(True)\n",
    "        \n",
    "        loss.backward()\n",
    "        print(model.desc.α.grad)\n",
    "        optimizer.step()\n",
    "        print(loss)\n",
    "\n",
    "    return loss_fn.lossE, loss_fn.lossG\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir='./20220727/copper_log')\n",
    "\n",
    "normalizer = Normalizer(tc.tensor(imgdataset.energies).double())\n",
    "\n",
    "for t in range(5000):\n",
    "    lossE, lossG = train(dataloader, model, MSEFLoss(), \n",
    "                         tc.optim.Adam(model.parameters(), lr=1e-2), normalizer)\n",
    "\n",
    "    writer.add_scalar('Loss / MSE energy (eV)', lossE, t)\n",
    "    writer.add_scalar('Loss / MSE grad (eV/A)', lossG, t)\n",
    "    if t % 10 == 0:\n",
    "        tc.save(model.state_dict(), './20220727/weights2_%d.pt' % t)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2bbbbc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.desc.α.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed687b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple",
   "language": "python",
   "name": "simple"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
