{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "189b15dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/schinavro/anaconda3/envs/simple/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from mlip.pes import PotentialNeuralNet\n",
    "from mlip.reann import REANN, compress_symbols\n",
    "\n",
    "species = [29]\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "lmax = 2\n",
    "nmax = 15\n",
    "loop = 2\n",
    "\n",
    "encode, decode, numbers = compress_symbols(species)\n",
    "species = list(set(numbers))\n",
    "reann = REANN(species, nmax=nmax, lmax=lmax, loop=loop)\n",
    "\n",
    "moduledict = nn.ModuleDict()\n",
    "desc = reann\n",
    "for spe in species:\n",
    "    moduledict[str(spe)] = nn.Sequential(\n",
    "        nn.Linear(desc.NO, int(desc.NO*1.3)),\n",
    "        nn.SiLU(),\n",
    "        nn.Linear(int(desc.NO*1.3), 1)\n",
    "    )\n",
    "moduledict = moduledict.double().to(device=device)\n",
    "    \n",
    "model = PotentialNeuralNet(desc, moduledict, species)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ce8847a",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dad8b641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pymatgen.core import Structure\n",
    "from monty.serialization import loadfn\n",
    "location = \"../data/Cu/\"\n",
    "data = loadfn(location + 'training.json')\n",
    "\n",
    "#data[0]['structure'].cart_coords;\n",
    "#data[0]['structure'].lattice.matrix;\n",
    "#data[0]['outputs']['forces'];\n",
    "#data[0]['structure'].lattice.pbc;\n",
    "#data[0]['num_atoms']\n",
    "\n",
    "symbols = [[encode[n] for n in d['structure'].atomic_numbers] for d in data]\n",
    "positions = [d['structure'].cart_coords for d in data]\n",
    "energies = [d['outputs']['energy'] for d in data]\n",
    "cells = [d['structure'].lattice.matrix for d in data]\n",
    "gradients = [-np.array(d['outputs']['forces']) for d in data]\n",
    "\n",
    "crystalidx = [[idx] * data[idx]['num_atoms'] for idx in range(len(data))]\n",
    "pbcs = [np.array(d['structure'].lattice.pbc) for d in data]\n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0faded7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nomenclature\n",
    "# SPECG-CriP(symbols, positions, energies, cells, gradients, crystalindex, pbcs)\n",
    "\n",
    "import torch as tc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BPTypeDataset(Dataset):\n",
    "    \n",
    "    \"\"\"Behler Parrinello Type datasets\n",
    "    Indexing should be done in the unit of crystal, a set of atom used in one calculation. \n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        symbols: List\n",
    "        positions: List\n",
    "        energies: List\n",
    "        cells: List\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, symbols, positions, energies, cells, gradients, crystalidx, pbcs):\n",
    "        self.symbols = symbols\n",
    "        self.positions = positions\n",
    "        self.energies = energies\n",
    "        self.cells = cells\n",
    "        self.gradients = gradients\n",
    "        self.crystalidx = crystalidx\n",
    "        self.pbcs = pbcs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.energies)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.symbols[idx], self.positions[idx], self.energies[idx], self.cells[idx], self.gradients[idx], self.crystalidx[idx], self.pbcs[idx]\n",
    "\n",
    "    \n",
    "def concate(batch, device='cpu'):\n",
    "    cat = lambda x: tc.from_numpy(np.concatenate(x))\n",
    "    \n",
    "    symbols, positions, energies, cells, gradients, crystalidx, pbcs = [], [], [], [], [], [], []\n",
    "    for data in batch:\n",
    "        symbol, position, energy, cell, gradient, crystali, pbc = data\n",
    "        symbols.append(symbol)\n",
    "        positions.append(position)\n",
    "        energies.append(energy)\n",
    "        cells.append(cell[None])\n",
    "        gradients.append(gradient)\n",
    "        crystalidx.append(crystali)\n",
    "        pbcs.append(pbc[None])      \n",
    "\n",
    "    return (cat(symbols), cat(positions).to(device=device).requires_grad_(True), \n",
    "            energies, cat(cells).to(device=device).requires_grad_(True), \n",
    "            cat(gradients), cat(crystalidx), cat(pbcs))\n",
    "\n",
    "imgdataset = BPTypeDataset(symbols, positions, energies, cells, gradients, crystalidx, pbcs)\n",
    "dataloader = DataLoader(imgdataset, batch_size=10, shuffle=True, collate_fn=concate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "663cfe22",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEFLoss:\n",
    "    def __call__(self, predE, predF, y, dy):\n",
    "        N = len(y)\n",
    "        A = len(dy)\n",
    "        self.lossE = tc.sum((y - predE) ** 2) / N\n",
    "        self.lossG = tc.sum((predF - dy)**2) / A\n",
    "        return self.lossE + self.lossG\n",
    "\n",
    "\n",
    "class Normalizer(object):\n",
    "    \"\"\"Normalize a Tensor and restore it later. \"\"\"\n",
    "\n",
    "    def __init__(self, tensor, device='cpu'):\n",
    "        \"\"\"tensor is taken as a sample to calculate the mean and std\"\"\"\n",
    "        self.mean = tc.mean(tensor).to(device=device)\n",
    "        self.std = tc.std(tensor).to(device=device)\n",
    "\n",
    "    def norm(self, tensor):\n",
    "        return (tensor - self.mean) / self.std\n",
    "\n",
    "    def denorm(self, normed_tensor):\n",
    "        return normed_tensor * self.std + self.mean\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {'mean': self.mean,\n",
    "                'std': self.std}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.mean = state_dict['mean']\n",
    "        self.std = state_dict['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a83a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Ge': 3? 'Li': 3, 'Mo': 42, 'Ni':28, 'Si': 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b4ce0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "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"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4f4cbb8a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(827949.6567, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(47408.4957, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(16957.3239, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4.2451e+15, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(42683.7181, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(100323.4514, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3.3221e+15, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.0363e+08, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.1238e+09, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.3722e+11, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(212449.2332, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(234602.6862, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.1062e+15, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(202908.3980, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(199802.7297, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(187960.6382, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(175114.3894, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.8464e+13, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(180372.0255, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3.5125e+08, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(171654.6827, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(174060.2940, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(170929.2796, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(172929.0230, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2.7070e+09, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(175777.6421, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(172516.0985, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(163191.0446, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(165423.9001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(151556.0261, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(127975.8674, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(93536.2335, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(48168.1800, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(113619.5234, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(19253.9724, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(9.4216e+10, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.2023e+13, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(57219.2247, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.0651e+16, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(87118.1272, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(106576.5103, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(106247.2412, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2.2965e+09, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(135412.4122, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(135885.2972, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(133426.9944, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.6720e+15, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(135280.8355, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.8518e+11, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6.9596e+15, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(138572.4732, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(140416.0001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(136698.8269, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(131247.9162, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(149179.0649, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7.5817e+09, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(117864.2656, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(114439.3978, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(12903128.0789, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3.7725e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(106850.2806, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(118378.8220, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(121151.3950, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(129574.2320, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(123731.6401, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(118296.4931, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.1290e+11, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(131685.2561, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(137371.8973, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(134688.2902, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(138558.9001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(139824.0196, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(139342.0882, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(142958.9346, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(133760.3962, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(141378.2830, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(152013.3664, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(162570.3947, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(147503.1389, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(468220.9321, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(136594.4264, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(150564.9784, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4499915.5239, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(137937.2878, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5.0042e+11, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(159286.8288, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(139521.5843, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(135342.1380, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(131969.2037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(126162.9633, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(125763.8444, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(122323.6919, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(436805.5263, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(117281.4972, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(443377.6908, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(115272.0568, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(110307.8835, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(116738.1635, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(115082.8391, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2672806.5881, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(106084.7869, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(111794.6464, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(107223.2876, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(100400.4279, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(13467647.4940, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(97737.5211, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(99658.3088, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(114322.0069, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(99772.2075, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(285808.6069, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(82477.4311, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(94062.8698, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(102452.2804, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(98177.6029, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(330345.0509, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(112670.2739, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(124169.5300, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(127839.2884, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(174439.2166, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(139922.7992, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4818008.6603, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(176629.1908, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(111331.3725, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(103371.4147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(100475.4826, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(97598.3692, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(100683.9035, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(97977.8154, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(101455.2819, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(95488.0992, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(204133.3647, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(97390.1728, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(78519.3005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(89265.7770, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(94104.9023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(90383.8254, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(76670.9357, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(61355.8844, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(57157.6979, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(53835.9532, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(23218.7622, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(110087.7516, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1791.9869, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(876.1884, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(13107.5455, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3560516.9874, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(8232.5931, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(187.2716, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(137927.3403, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5291.6791, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(25924.8072, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(15035.5630, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(10980.8990, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(15614.3508, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(20015.2057, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(22102.5969, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(27170.6387, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(962277.8884, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(18573.9679, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(26727.1023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(93326.6120, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(16519.6831, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(24149.7005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7401.0587, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1641.1851, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(27320.8354, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(477020.1690, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2163.9254, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(578.7396, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2768.6408, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5384.0903, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5120.1989, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(762.3083, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(12959.3411, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(11859.2045, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(17703.7720, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2899.0466, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1482.0064, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1763.8509, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1991.1333, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(94226.5013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(228727.8534, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(28644.6840, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(40741.3441, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(16482.4965, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(17017.4334, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(14870.5686, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(12279.2376, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7682.2257, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(32913.3233, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(24833.0231, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1512.4225, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(16757.1041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(13214.6254, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2354.8772, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2292.9768, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(14310.3109, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4934.0864, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(662.8773, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1699.6693, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5284.0266, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6157.0092, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2990.4293, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(17561.5073, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(52387.7815, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(403820.5504, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(149913.4994, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(188.3889, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7077.5119, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(16152.5704, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(24070.8544, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(28511.4986, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(30817.9124, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(50093.1976, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(19591.6151, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(8335.5938, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(33071.2964, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2614.1427, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3558.3235, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1472.7977, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(55680.2960, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(587.7523, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(652.8443, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(10378.1669, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1298.0439, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1219.7210, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(375707.5692, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2278.0582, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(18367.5495, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(179956.3460, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2292.4278, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(684.0535, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(137.1823, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(793.3964, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1267.6368, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(613.7165, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2395.3296, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(27931.6618, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(313213.6256, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(94.7235, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(17914.2903, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(501.6106, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1207.3068, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(564.1738, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(16606.6377, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1892.0066, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3681.8772, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(8335.6180, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(11122.9484, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(8873.3963, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(17200.8733, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1343.3476, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(272.0298, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2210.1857, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4651.5735, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(17167.7176, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2172.7645, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(661.1224, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(94045.2921, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(10966.7426, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(45125.6241, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1367.7166, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(621.4284, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(846.4291, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(20210.0484, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(615.7365, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(969.7887, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1331.0223, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(79076.9558, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1024.6901, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(171.8444, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(15561.1240, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1295.5612, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1999.4936, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(21520.4405, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6479.1106, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(20864.9850, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(15317.6903, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7509.1090, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4295.5681, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(20061.8630, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1125.2799, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(12228.5796, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2934.2946, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3712.4237, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2443.5020, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(53739.8698, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(21683.3466, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(580.1354, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1053.5612, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1148.4678, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2317.7549, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2034.7294, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2049.8952, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1581.3471, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(9489.3873, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(82527.7049, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(12777.4211, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(14290.0642, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2337.1728, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(585.9952, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3784.7566, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(26645.9309, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(522.0727, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(613.6805, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(741.7851, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4870993.3501, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(490506.8536, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(583.7209, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1186.0565, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1123.0772, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(16317.5720, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(874.6149, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1161.3512, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1759.3672, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(818.5573, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(924.4879, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(673.4395, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(23425.7312, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1280.2260, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1530.2472, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1305.6333, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1214.3737, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2357.6367, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1120.1526, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1915.5595, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1436.3866, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(389.1292, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(401.3068, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(777837.4422, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(482701.0936, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(11982.9307, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1317.0366, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(888.5664, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(766.1661, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(740.2153, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(805.3331, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(77662.8047, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(663.5511, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(22508.0306, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(262818.7389, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1178.1844, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1780.1258, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1046.3289, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1744.1131, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(38438.3825, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(950.3461, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(590.9898, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(793.2254, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(548.7657, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1495.8023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1088.5255, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(33029.6728, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1854.2645, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(13304.6585, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3228.0227, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1455.0545, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1101.6081, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(9010.7424, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1547.7180, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1120.4539, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(44479.0577, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1111.5937, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3818.9968, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3383.0788, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(915.6568, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(12735.1976, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(15442.2669, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(329343.3750, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2100.4097, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2001.6866, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2340.7958, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(408.7065, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(605.1593, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(15344.3347, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(633.1978, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(900.9665, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1111.4923, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1161.8175, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(22276.9352, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5267.9194, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(608.5531, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1506.6334, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2472.7127, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4186.2275, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3840.4754, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1664.7016, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(531.0061, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(776.2338, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2594.8852, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(15546.2890, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1480.5407, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(151.3657, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(283.1264, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(901.4854, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(24928.8394, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1623.2934, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(127507.5515, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2647.0829, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2755.1259, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(24547.6192, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(20140.3827, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3230.1656, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(18266.7345, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(832.9746, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2520.1746, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(10316.9355, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3165.9517, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1119.8342, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(14444.4677, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3541.2427, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4507.3785, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2715.5683, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1353.9679, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(31826.9655, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(51275.0287, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(18776.7077, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6376.5036, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5861.8928, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(8380.1868, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(672.8260, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(370.3995, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(632.1900, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(52156.4793, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2770.3944, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4911.7181, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4502.4586, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4443.0131, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1075.3104, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(841.2627, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(629.2776, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(369.0988, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7532.4192, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(24872.3575, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1228.8269, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1006.8957, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(818.7543, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(244.4734, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5008.0727, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(16208.4453, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(25032.3887, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1497.8429, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(115.5247, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(805.8666, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(575.8547, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(659.5316, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1109.2114, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(347784.2377, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2053.1011, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3579.0896, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4412.5892, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6411.6642, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(38139.1673, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6844.7866, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7867.4092, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4308.9310, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(28852.4492, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2756.5344, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2586.8964, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4811.2385, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3550.2431, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(176.3647, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(845.5488, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1029.9145, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(865.9855, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(776.0525, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(237562.4064, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(13600.2389, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(642.9746, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(15215.9733, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(19445.4690, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1506.7737, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1153.9474, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7071.7994, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1084.0699, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(15321.1360, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1407.7244, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1148.5744, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1543.8657, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(830.5504, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(465.7361, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(575.6197, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(299.3445, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(167138.8064, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(21252.9406, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(173.3624, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(30409.1241, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(500.9776, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1365.2593, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1154.0218, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1358.9995, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(474.8641, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(101.5252, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2907.9597, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1080357.6077, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(51149.4749, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(852.6597, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(608.3624, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1145.3670, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3975.8413, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(312106.2220, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2900.8188, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2223.3577, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4319.8666, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1553.0930, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1188.4824, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2348.0088, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2021.5815, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1316.8775, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1106.6211, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1661.5634, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1290.6664, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4328.9862, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1.4202e+14, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2608.0739, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(28390.3366, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(16867.3809, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7074.1890, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7560.6800, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(9021.1302, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(29445.3671, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(12723.4111, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(14467.8823, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(15763.4671, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(14837.2676, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(16939.4901, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(15044.0514, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(13471.0663, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(14932.8934, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(17327.4582, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(16928.0184, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(14519.7644, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(14721.1463, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(14794.0204, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(14953.4948, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(15708.8163, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(16628.6400, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(13500.1664, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(15486.9953, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(18100.4335, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(16673.6009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(59583.8611, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(9570.3888, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(10644.3332, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7418.1828, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(48172.6955, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5261.6587, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4264.1578, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4174.8288, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4831.0548, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2663.3405, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(15487.9331, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(39047.9166, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(738.3860, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(19921.2494, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5410.3121, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1295.8254, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1430.3218, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2130.8874, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(32255.3605, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(986.8071, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3973.8313, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4691.8764, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(763.9807, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1448.2937, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2219.3435, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2396.7994, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1606.1523, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4826.7115, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1999.3290, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(455.5651, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4013.3689, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(36454.0469, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(833.1883, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(14408.6826, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1103.7265, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1467.1218, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(40152.9285, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1404.1010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1694.8464, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(21895.1028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(11673.1350, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7781.4693, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(682.0885, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(958.0867, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(770.1545, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(591.4124, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(18443.0068, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(17166.6113, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(585.2159, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5698.5367, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1020.1924, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(390.6052, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2656.3054, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(323.2792, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3226.5840, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1637.5433, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1214.7326, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(32147.6167, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2738.3867, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(28556.9228, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(59849.1485, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1156.3886, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1003.9002, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(717.5585, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(823.2038, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(566.1141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(13259.7869, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(14953.4721, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(400.1998, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(315.5291, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(358.5475, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(25089.3539, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(522.5101, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(469.8617, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(763.5158, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1343.0313, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(745.6511, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1086.2956, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(792.5587, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(326.9055, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(457.0659, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5687.1758, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1158.6112, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(254.3698, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(15192.9166, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1182.9399, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(13817.9382, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(11555.5318, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(8777.0992, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(684.3942, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(403.5079, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(21131.6783, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(235.3973, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(364.0067, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(16876.6772, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(965.9264, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(512.2496, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(800.5432, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(346.6747, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(115543.9076, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(669.1255, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(699.2995, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(163.7120, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(927.9399, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(850.0265, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(394.4777, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(9.9714, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(648.7228, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(612.3908, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(656.1632, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(492.2902, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(456.4136, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(41278.2548, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(31974.6984, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(14389.7041, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6590.8550, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(563.2662, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(12600.3917, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(387.0180, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(441.8925, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(799.6448, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(446.4835, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(298.4256, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1176.4690, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(537.2718, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(536.3906, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1041.1823, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(35145.7426, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(590.9108, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(466.6557, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(736.2365, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(29701.3271, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(589.0118, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1069.6994, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1241.5446, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(468.8991, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(35706.4108, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(250.5210, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(621.4590, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(818.6732, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(669.6188, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(439.8338, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(500.3369, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(9280.0073, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5096.1847, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(14159.4116, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(308.4984, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(723.6492, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5004.6898, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1902.2688, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(18290.2872, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(163.6503, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(326.3549, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(11891.3678, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2474.0640, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1063.2507, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(17402.2871, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1386.9170, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(761.6466, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1027.9883, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(201.1849, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(13740.8363, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(683.2953, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(239.5312, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(31391.5975, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(429.8535, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(9237.1904, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(13811.8796, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4955.7825, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1638.7105, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(128.2232, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1508.6427, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(471.3657, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(34990.1003, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1350045.9879, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(631.8592, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1001.3355, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(561.6314, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(547.7771, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(774.0869, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(415.1532, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(379.1656, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5456.9432, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1431.4550, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(302.9763, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(469.7970, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(29205.8968, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(100.8863, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(605.8525, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1060.0196, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(596.2776, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(28025.6830, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(169.6492, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(736.9472, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(217.2607, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(249.9514, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(17544.1321, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(333.3019, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(19523.8888, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(709.5455, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(327.5023, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(675224.1028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1244.6993, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1838.5561, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(11358.1814, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1466.7757, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5399.5724, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1629.1494, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(582.2494, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(351.8217, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(326.3945, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(19163.4183, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5540.6976, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(732.0158, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(537.8326, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5866.4753, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(606.8444, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(327.8323, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(698.9029, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(363.1014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1048.0154, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(807.3269, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(786.9539, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3473.7894, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(587.1585, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2561.9734, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(556.4778, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(8351.1676, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(482.7883, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(11350.3443, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5385.9168, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(37951.5252, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(627.7981, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(268.7778, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(360.6403, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1196.2400, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1341.3119, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4371.4064, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(783.6622, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2952.3239, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(15193.9292, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(72.4251, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(281.0676, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(472.8525, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(8267.0662, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2068.2774, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6019.5085, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1750.3328, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(965.4887, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(926.3112, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(9467.2025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1050.6328, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(750.9865, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1356.3052, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(405.5741, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1408.3753, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(416.1682, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(454.8643, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(340.7293, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(441.1206, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(17419.4966, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(464.2588, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(366.1324, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1460.3477, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(669.5202, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(264.5471, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1158.9883, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(100175.9110, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3335.0497, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(372.2539, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4892.4460, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(699.6762, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(889.0240, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2500.0361, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2883.5364, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(25630.5555, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1685.7762, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(987.9647, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1765.1779, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(526.2979, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1445.1505, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(537.0586, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1975.4943, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(11299.4369, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1181.0406, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(427.2644, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1079.3767, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3255.8673, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(418.8200, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(362.4683, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(590.5922, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(413.6037, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(11294.3513, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(565.9322, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(542917.7520, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1768.0936, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(836.3673, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(17580.7823, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1302.6617, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(889.3924, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(675.2742, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5379.2348, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(817.2566, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2232.4340, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(714.9966, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1405.5570, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1190.6663, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(959.4426, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(21329.9043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(481.1101, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(136.0708, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(427.4697, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(494.8134, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(950.0691, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(450.4292, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(23583.8098, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4716.6915, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(467.6941, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(524.4856, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(729.5638, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(391.0903, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(802.4644, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(119.6816, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3364.6158, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(732.3008, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(948.4109, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(788.1120, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(723.7678, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(674.1153, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(410.0696, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1588.9185, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(102.9298, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(66927.2349, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2941.0858, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(16122.2658, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(434.8707, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(329.5766, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(296.1549, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(141.7832, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(16257.1558, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(40832.0149, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(597.8630, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(22750.9877, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4292.2219, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1095.1949, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(852.3542, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1084.7903, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(887.1585, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1212.2147, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(483.9396, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2362364.1349, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1572.0659, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(27875.5781, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(12446.9074, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2707.3549, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2760.5108, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3568.4669, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(13118.1870, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2058.3076, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2075.2314, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3039.6385, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1903.5764, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1354.8699, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1007.9911, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(804.3274, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1133.3059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(750.7288, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(466.9135, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(979.1032, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(728.0676, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(32927.4338, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(242.7130, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(493.7997, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1064.3206, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(10900.9821, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(747.0182, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(380.4669, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(668.4004, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1361.0656, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(760.0463, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1212.8523, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(388.8663, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(266.3026, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(26793.5855, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(526.3281, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(371.9696, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5434.5978, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1930.1137, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7164.3900, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(286.5671, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(492.0110, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1101.9364, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(588.9756, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(560.4759, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(769.5141, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(715.6294, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3331.6954, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(437.7498, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(355.3587, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(378.6119, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1391.0317, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(427.8059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(19254.7424, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(81.4118, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(446.8536, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(16773.4180, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2265.5866, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1065.3967, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(536.7103, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2173.0693, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1818.2441, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1060.5288, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(147.4948, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(183.5903, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(170.3370, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(854.0113, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1606.6597, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1471.0987, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(364.9864, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(964.8503, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(691.9046, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5809.4075, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(20365.5295, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(442.1238, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4649.5788, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1258.9809, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1601.6719, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1233.3833, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(34282.7909, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(10920.8507, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(548.0190, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(8652.3209, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7044.9076, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(515.1905, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(591.1639, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2326.0387, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1755.2902, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(208.6533, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(468.9296, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(582.3366, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4214.1923, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(25720.2488, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(371.9014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(843.0533, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1046.8519, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(142552.9663, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3272.7063, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(422.6157, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1159.9446, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3204.6429, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(280.2536, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(826.3796, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(553.7220, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(477.8398, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(533.1350, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(720.0005, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(617.7946, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(258.4130, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(301.7939, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(299130.4384, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(170.9776, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(602.7725, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1610.7774, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1507.3328, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(278.8596, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(767.1439, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(658.0415, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(377.9785, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3543.0843, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4139.0426, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(757.0762, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(309.4339, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(382.9811, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(570.2054, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(15700.4456, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(525.4010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(508.3083, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(985.4700, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1010.4477, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3434.0344, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(303.6832, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(9101.0376, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1149.2494, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(727.1708, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(159.6009, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7504.8566, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1954.9493, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(108.2259, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(596.0983, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(19145.1404, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1839.3404, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(687.2607, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(848.7232, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(585.0121, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3965.9080, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(738.7152, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(761.7190, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1571.8177, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(621.1913, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(923.3955, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(323.5882, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(576.6990, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(660.7097, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(19023.1852, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1159.0373, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(899.4693, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(875.9906, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(646.8716, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(283.3048, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4065.1658, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(430.2664, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(19623.0302, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(737.6177, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(995.3651, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(402.3510, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1406.3970, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(616.1778, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(443.9810, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4730.2820, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(374.9386, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(118.6526, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1422.3153, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(420.7452, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1049.7770, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(168.7854, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1049.3107, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(423.1728, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(439.5414, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(562.2301, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(943.1458, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(324.1404, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(835.4375, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(643.6389, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(662.5662, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(198.1611, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(11821.0396, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(415.9518, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(439.5522, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(605.0536, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1447.8305, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1767.8718, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1447.6091, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(676.9715, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(113.6365, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3796.2281, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6414.4731, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(509.9014, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(897.4365, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(930.9356, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(647.7785, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1278.5111, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(315.4736, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(779.2712, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(353.5409, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(581.3314, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(173.5597, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1436.6979, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2653.0626, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(597.0200, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(543.3101, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(55707.1720, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2828.4447, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(610.3994, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(84.9763, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(536.6419, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1532.0028, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(725.2215, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(717.8818, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(571.8941, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(940.5705, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(266.7734, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(517.4259, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(393.5848, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(954.5198, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(754.3362, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(648.9305, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(119.6659, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(20219.1388, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(95.3814, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(696.9082, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2574.8077, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(835.4256, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(111.3733, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(334.8420, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1370.5374, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1020.4951, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(193.6619, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(549.3310, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(22167.1945, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1319.8078, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3151.6788, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(140.8701, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(108.0724, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(553.9879, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1021.0496, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(693.7713, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(470.0603, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(96.2066, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(88.7735, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(398.7099, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1879.8773, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(845.0719, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1350.3835, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(596.7213, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(579.5554, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(10616.0887, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(522.2885, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(367.8629, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4581.3227, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(925.0673, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(39103.2113, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1124.1264, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(657.5204, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(826.3827, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5300.3533, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(659.1498, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(521.6830, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(351.7525, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(10438.0928, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(18.0847, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2787.6570, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(176.7209, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(755.4500, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1136.9952, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(735.1262, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(352049.1071, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(686.4694, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(519.7058, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(331301.2086, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(317.3700, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1775.3212, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(932.0532, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(797.5378, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5713.5525, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(404.2081, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(693.5414, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(941.1886, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3683.9596, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1651.4084, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4896.7803, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(633.7998, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(566.3309, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(608.9294, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7707.8860, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(450.6033, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(522.3171, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2890.4114, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1158.8025, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4680.1186, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1338.0410, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(729.7336, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(468.2043, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4120.6641, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(664.2492, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(257.8920, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(403.3786, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(707.2714, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(500.9252, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(567.8030, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(370.1872, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(388.2581, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1032.0601, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(85.1034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(465.6152, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(287.3166, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(377.5379, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(824.9987, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(19072.9202, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1974181.3808, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(721.8001, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(789.1558, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2077.1302, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(582.1244, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(60.0017, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(207.8878, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4644.7525, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(531.2410, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(929.8619, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(657.4632, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3456.4817, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(14418.4787, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(538.4642, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(482.6763, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(819.6252, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1428.7285, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1095.2494, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(957.8145, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(913.1112, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(453331.2385, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1287.6186, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4221.1738, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6985.5325, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(585.5142, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(673.9310, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(800.4888, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5784.0232, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3370.7612, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1061.8183, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(10799.3527, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1024.8383, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2095.2771, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6554.5932, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(923.3410, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(23441.1679, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(10540.8591, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4812.6815, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1227.9387, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1200.6755, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(852.6924, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(10610.5661, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3172.4133, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(192.6090, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(238.9360, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(929.3231, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(436.9018, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5235.3460, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(34669.2709, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1783.8794, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1271.1976, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3354.7220, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(328.6207, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5727.6860, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(822.5491, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(563.3059, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(802.7963, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(663.5333, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(674.0191, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(634.7622, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(536.5347, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2498.5935, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(327.2183, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2779.9763, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3765.2359, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(226.0552, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1802.9596, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(556.7709, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(489.0291, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1029.6357, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1443.2463, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1985.7424, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(12995.1860, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1003.7326, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(543.4386, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(635.3630, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(251.6810, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6772.2518, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(155.5295, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(659.9170, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(414.3158, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(653.3457, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(863.2728, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(823.9433, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3818.0529, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7401.0620, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(694.8336, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(619.4386, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(143.8375, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(451.7538, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1036.0034, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(7607.3818, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5218.6589, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(552.2919, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(324.8462, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(61086.4376, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(705.8871, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1610.3264, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(644.7486, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(789.7701, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1779.0672, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1275.2761, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(521.2565, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(563.5145, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(450.5495, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(473.8510, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(24132.2311, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6318.7634, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(511.0741, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(16104.1691, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(865.8211, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(250.1564, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(493.4697, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(819.2047, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(599.4655, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(711.0920, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1873.1262, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(421.5626, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2264.2681, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1074.7462, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1270.5576, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(727.7737, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(429.7513, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(3111.2234, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(733.8325, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(592.3416, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(858.7360, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(258.0326, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4804.0815, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(430.4743, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(5600.4022, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(19488.5554, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4606.4006, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(664.4013, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(576.6956, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2090.5920, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1223.1642, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1184.5437, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1595.4402, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(2459.9513, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(523.1791, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1691.2930, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(545.4884, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(48489.7812, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(55478.1884, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(275.6050, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(609.0912, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(6987.1242, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(784.4251, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(8783.0010, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1402.1670, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4181.0305, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(926.3915, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(1009.1721, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(791.5449, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(890.4273, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(867.0305, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(862.0681, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(593.0602, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(14609.9574, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(380.6759, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(4908.3830, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(380.4584, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(839.0730, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(659.3917, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(308.4216, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(734.0124, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(12.5683, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(628.6834, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(513.0675, dtype=torch.float64, grad_fn=<AddBackward0>)\n",
      "tensor(677.2213, dtype=torch.float64, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 31>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m normalizer \u001b[38;5;241m=\u001b[39m Normalizer(tc\u001b[38;5;241m.\u001b[39mtensor(imgdataset\u001b[38;5;241m.\u001b[39menergies)\u001b[38;5;241m.\u001b[39mdouble())\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m5000\u001b[39m):\n\u001b[0;32m---> 32\u001b[0m     lossE, lossG \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMSEFLoss\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss / MSE energy (eV)\u001b[39m\u001b[38;5;124m'\u001b[39m, lossE, t)\n\u001b[1;32m     36\u001b[0m     writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss / MSE grad (eV/A)\u001b[39m\u001b[38;5;124m'\u001b[39m, lossG, t)\n",
      "Input \u001b[0;32mIn [8]\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(dataloader, model, loss_fn, optimizer, normalizer, device)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[1;32m     11\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 13\u001b[0m _, pred, predG \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcells\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrystalidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbcs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m#loss = loss_fn(pred, predG, normalizer.norm(tc.tensor(energies)), gradients)\u001b[39;00m\n\u001b[1;32m     16\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(pred, predG, tc\u001b[38;5;241m.\u001b[39mtensor(energies), gradients)\n",
      "File \u001b[0;32m~/anaconda3/envs/simple/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/simple/lib/python3.8/site-packages/mlip/pes.py:66\u001b[0m, in \u001b[0;36mPotentialNeuralNet.forward\u001b[0;34m(self, symbols, positions, cells, crystalidx, pbcs, cutoff)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m\"\"\" \u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m \n\u001b[1;32m     63\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# Descriptor calculation\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# NTA x 3 -> NTA x D\u001b[39;00m\n\u001b[0;32m---> 66\u001b[0m desc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdesc\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcells\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrystalidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbcs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     68\u001b[0m positionsidx \u001b[38;5;241m=\u001b[39m tc\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;28mlen\u001b[39m(positions))\n\u001b[1;32m     69\u001b[0m energies, new_positionsidx \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/simple/lib/python3.8/site-packages/torch/nn/modules/module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1098\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1099\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1103\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1104\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/simple/lib/python3.8/site-packages/mlip/reann.py:275\u001b[0m, in \u001b[0;36mREANN.forward\u001b[0;34m(self, symbols, positions, cells, crystalidx, pbcs)\u001b[0m\n\u001b[1;32m    253\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m----------\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;03m    density \u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Number of crystals, Number of total atoms\u001b[39;00m\n\u001b[1;32m    274\u001b[0m iidx, jidx, isym, jsym, disp, dist \u001b[38;5;241m=\u001b[39m \\\n\u001b[0;32m--> 275\u001b[0m     \u001b[43mget_neighbors_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbols\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpositions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcells\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrystalidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbcs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    277\u001b[0m NTA \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(positions)\n\u001b[1;32m    278\u001b[0m dtype, device \u001b[38;5;241m=\u001b[39m positions\u001b[38;5;241m.\u001b[39mdtype, positions\u001b[38;5;241m.\u001b[39mdevice\n",
      "File \u001b[0;32m~/anaconda3/envs/simple/lib/python3.8/site-packages/mlip/reann.py:151\u001b[0m, in \u001b[0;36mget_neighbors_info\u001b[0;34m(symbols, positions, cells, crystalidx, pbcs, cutoff)\u001b[0m\n\u001b[1;32m    149\u001b[0m pbc, cell \u001b[38;5;241m=\u001b[39m pbcs[c], cells[c]\n\u001b[1;32m    150\u001b[0m \u001b[38;5;66;03m# NN, NN, NNx3, NN\u001b[39;00m\n\u001b[0;32m--> 151\u001b[0m idx, jdx, isy, jsy, dsp, dst \u001b[38;5;241m=\u001b[39m \u001b[43mget_nn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msymbol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpbc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    152\u001b[0m iidx\u001b[38;5;241m.\u001b[39mappend(crystali[idx])\n\u001b[1;32m    153\u001b[0m \u001b[38;5;66;03m# iidx.append(idx)\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/simple/lib/python3.8/site-packages/mlip/reann.py:85\u001b[0m, in \u001b[0;36mget_nn\u001b[0;34m(symbols, positions, cell, pbc, cutoff, device)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# AxA\u001b[39;00m\n\u001b[1;32m     84\u001b[0m dist1 \u001b[38;5;241m=\u001b[39m tc\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39mnorm(disp1, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m---> 85\u001b[0m dist2 \u001b[38;5;241m=\u001b[39m \u001b[43mtc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdisp2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# AxA\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n1 \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m n2 \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m n3 \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch as tc\n",
    "from torch.autograd import grad\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, normalizer, device='cpu'):\n",
    "    model.train()\n",
    "    for batch, _ in enumerate(dataloader):\n",
    "\n",
    "        symbols, positions, energies, cells, gradients, crystalidx, pbcs = _\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        _, pred, predG = model(symbols, positions, cells, crystalidx, pbcs)\n",
    "        \n",
    "        #loss = loss_fn(pred, predG, normalizer.norm(tc.tensor(energies)), gradients)\n",
    "        loss = loss_fn(pred, predG, tc.tensor(energies), gradients)\n",
    "        \n",
    "        loss.requires_grad_(True)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(loss_fn.lossE.data, loss_fn.lossG.data)\n",
    "\n",
    "    return loss_fn.lossE, loss_fn.lossG\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter(log_dir='./20220727/copper_log')\n",
    "\n",
    "normalizer = Normalizer(tc.tensor(imgdataset.energies).double())\n",
    "\n",
    "for t in range(5000):\n",
    "    lossE, lossG = train(dataloader, model, MSEFLoss(), \n",
    "                         tc.optim.Adam(model.parameters(), lr=1e-2), normalizer)\n",
    "\n",
    "    writer.add_scalar('Loss / MSE energy (eV)', lossE, t)\n",
    "    writer.add_scalar('Loss / MSE grad (eV/A)', lossG, t)\n",
    "    if t % 10 == 0:\n",
    "        tc.save(model.state_dict(), './20220727/weights_%d.pt' % t)\n",
    "\n",
    "writer.flush()\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9439b3b6",
   "metadata": {},
   "source": [
    "127810 cas_v100_ Serial_g x2419a03 PD       0:00      1 (Priority)\n",
    "127809 cas_v100_ Serial_g x2419a03 PD       0:00      1 (Priority)\n",
    "127808"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4294f0",
   "metadata": {},
   "source": [
    "H0Y123700PNPH4a2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple",
   "language": "python",
   "name": "simple"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
