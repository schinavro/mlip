{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/x2419a03/.conda/envs/simple/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "from mlip.pes import PotentialNeuralNet\n",
    "from mlip.reann import REANN, compress_symbols\n",
    "\n",
    "def gen_module(species=None, moduledict=None, modulelist=None, nmax=2, lmax=10, loop=2, rcut=6.0, device='cpu'):\n",
    "\n",
    "    encode, decode, numbers = compress_symbols(species)\n",
    "    species = list(set(numbers))\n",
    "    reann = REANN(species, modulelist=modulelist, nmax=nmax, lmax=lmax, loop=loop, device=device, rcut=rcut)\n",
    "    \n",
    "    return PotentialNeuralNet(reann, moduledict, species)\n",
    "model = gen_module(species=[29], lmax = 2, nmax = 15, loop = 2, device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nomenclature\n",
    "# SPEC-PECriG(symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients)\n",
    "\n",
    "import torch as tc\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class BPTypeDataset(Dataset):\n",
    "    \n",
    "    \"\"\"Behler Parrinello Type datasets\n",
    "    Indexing should be done in the unit of crystal, a set of atom used in one calculation.\n",
    "    \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "        symbols: List\n",
    "        positions: List\n",
    "        energies: List\n",
    "        cells: List\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients):\n",
    "        self.symbols = symbols\n",
    "        self.positions = positions\n",
    "        self.energies = energies\n",
    "        self.cells = cells\n",
    "        self.pbcs = pbcs\n",
    "        self.energyidx = energyidx\n",
    "        self.crystalidx = crystalidx\n",
    "        self.gradients = gradients\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.energies)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.symbols[idx], self.positions[idx], self.energies[idx], self.cells[idx], self.pbcs[idx], self.energyidx[idx], self.crystalidx[idx], self.gradients[idx]\n",
    "\n",
    "    \n",
    "def concate(batch, device='cuda'):\n",
    "    cat = lambda x: tc.from_numpy(np.concatenate(x))\n",
    "    \n",
    "    symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients = [], [], [], [], [], [], [], []\n",
    "    for data in batch:\n",
    "        symbol, position, energy, cell, pbc, energyid, crystalid, gradient = data\n",
    "        symbols.append(symbol)\n",
    "        positions.append(position)\n",
    "        energies.append(energy)\n",
    "        cells.append(cell[None])\n",
    "        pbcs.append(pbc[None])      \n",
    "        energyidx.append(energyid)\n",
    "        crystalidx.append(crystalid)\n",
    "        gradients.append(gradient)\n",
    "\n",
    "    return (cat(symbols), cat(positions).to(device=device).requires_grad_(True), \n",
    "            tc.tensor(energies).to(device=device), cat(cells).to(device=device).requires_grad_(True), \n",
    "            cat(pbcs), tc.tensor(energyidx).to(device=device), cat(crystalidx).to(device=device), cat(gradients).to(device=device))\n",
    "\n",
    "import numpy as np\n",
    "from pymatgen.core import Structure\n",
    "from monty.serialization import loadfn\n",
    "\n",
    "def get_dataloader(location, symbol, number, name, batch_size=10):\n",
    "\n",
    "    data = loadfn(location + symbol + '/' + name)\n",
    "    encode, decode, numbers = compress_symbols([number])\n",
    "\n",
    "    #data[0]['structure'].cart_coords;\n",
    "    #data[0]['structure'].lattice.matrix;\n",
    "    #data[0]['outputs']['forces'];\n",
    "    #data[0]['structure'].lattice.pbc;\n",
    "    #data[0]['num_atoms']\n",
    "    symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients  = [], [], [], [], [], [], [], []\n",
    "    for idx, d in enumerate(data):   \n",
    "#        if d['outputs']['energy'] > -400:\n",
    "#            continue\n",
    "        symbols.append([encode[n] for n in d['structure'].atomic_numbers])\n",
    "        positions.append(d['structure'].cart_coords)\n",
    "        energies.append(d['outputs']['energy'])\n",
    "        cells.append(d['structure'].lattice.matrix)\n",
    "        pbcs.append(np.array(d['structure'].lattice.pbc))\n",
    "        energyidx.append(idx)\n",
    "        crystalidx.append([idx] * data[idx]['num_atoms'])\n",
    "        gradients.append(-np.array(d['outputs']['forces']))\n",
    "\n",
    "    imgdataset = BPTypeDataset(symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients)\n",
    "    \n",
    "    return imgdataset, DataLoader(imgdataset, batch_size=batch_size, shuffle=True, collate_fn=concate)\n",
    "\n",
    "location = \"/home01/x2419a03/libCalc/mlip/data/\"\n",
    "symbol = 'Cu'\n",
    "number = 29\n",
    "imgdataset, dataloader = get_dataloader(location, symbol, number, 'training.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEFLoss:\n",
    "    def __init__(self, muE=1., muF=1.):\n",
    "        self.muE = muE\n",
    "        self.muF = muF\n",
    "    def __call__(self, predE, predF, y, dy):\n",
    "        #_, NA = tc.unique_consecutive(crystalidx, return_counts=True)\n",
    "        #self.NTA = len(dy)\n",
    "        #self.lossE = tc.sqrt(tc.sum(((y - predE)/ NA)**2) / len(y))\n",
    "        #self.lossG = tc.sqrt(tc.sum((predF - dy)**2) / self.NTA)\n",
    "        self.lossE = tc.sum((y - predE)**2)\n",
    "        self.lossG = tc.sum((dy - predF)**2)\n",
    "        return self.muE * self.lossE + self.muF * self.lossG\n",
    "\n",
    "\n",
    "class Normalizer(object):\n",
    "    \"\"\"Normalize a Tensor and restore it later. \"\"\"\n",
    "\n",
    "    def __init__(self, tensor, device='cuda'):\n",
    "        \"\"\"tensor is taken as a sample to calculate the mean and std\"\"\"\n",
    "        self.mean = tc.mean(tensor).to(device=device)\n",
    "        self.std = tc.std(tensor).to(device=device)\n",
    "\n",
    "    def norm(self, tensor):\n",
    "        #return (tensor - self.mean) / self.std\n",
    "        return tensor - self.mean\n",
    "\n",
    "    def denorm(self, normed_tensor):\n",
    "        #return normed_tensor * self.std + self.mean\n",
    "        return normed_tensor + self.mean\n",
    "\n",
    "    def state_dict(self):\n",
    "        return {'mean': self.mean,\n",
    "                'std': self.std}\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.mean = state_dict['mean']\n",
    "        self.std = state_dict['std']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 : 65.65589   1.60159    81.64703   1.63421   \n",
      "1 : 55.67689   0.60073    74.38272   0.31040   \n",
      "2 : 43.30691   0.25400    67.90010   0.32894   \n",
      "3 : 41.30679   0.24755    66.43029   0.27920   \n",
      "4 : 40.64006   0.22535    65.89469   0.25286   \n",
      "5 : 40.59384   0.21783    65.93821   0.25019   \n",
      "6 : 40.61877   0.21183    65.96888   0.23780   \n",
      "7 : 40.62689   0.20589    65.85636   0.23277   \n",
      "8 : 40.63557   0.20163    65.91538   0.22372   \n",
      "9 : 40.62475   0.19683    65.96876   0.21758   \n",
      "10 : 40.59098   0.19391    65.93519   0.21552   \n",
      "11 : 40.59460   0.18807    65.91565   0.20987   \n",
      "12 : 40.58035   0.18415    65.96486   0.20309   \n",
      "13 : 40.56656   0.18057    65.83937   0.20142   \n",
      "14 : 40.56643   0.17492    65.93661   0.19432   \n",
      "15 : 40.57292   0.16967    65.87769   0.18565   \n",
      "16 : 40.55442   0.16574    65.90156   0.18385   \n",
      "17 : 40.55238   0.16088    65.87539   0.17723   \n",
      "18 : 40.54974   0.15669    65.91172   0.17126   \n",
      "19 : 40.55404   0.15261    65.87011   0.16782   \n",
      "20 : 40.54065   0.14710    65.91581   0.15941   \n",
      "21 : 40.50935   0.14476    65.83739   0.15987   \n",
      "22 : 40.52478   0.13888    65.85910   0.15096   \n",
      "23 : 40.52409   0.13453    65.80300   0.14697   \n",
      "24 : 40.53424   0.13009    65.81557   0.14124   \n",
      "25 : 40.52371   0.12509    65.80827   0.13429   \n",
      "26 : 40.49850   0.12067    65.84202   0.13054   \n",
      "27 : 40.49707   0.11684    65.78411   0.12591   \n",
      "28 : 40.48970   0.11239    65.83890   0.12008   \n",
      "29 : 40.53658   0.10818    65.79003   0.11450   \n",
      "30 : 40.47345   0.10466    65.77013   0.11124   \n",
      "31 : 40.47215   0.10032    65.82627   0.10653   \n",
      "32 : 40.51023   0.09623    65.81111   0.10064   \n",
      "33 : 40.46296   0.09352    65.80897   0.10074   \n",
      "34 : 40.45752   0.09331    65.80165   0.10099   \n",
      "35 : 40.45337   0.09314    65.80341   0.10100   \n",
      "36 : 40.45135   0.09278    65.80400   0.10054   \n",
      "37 : 40.45046   0.09243    65.81211   0.10028   \n",
      "38 : 40.44902   0.09209    65.79272   0.09994   \n",
      "39 : 40.44671   0.09177    65.79703   0.09965   \n",
      "40 : 40.44784   0.09136    65.77935   0.09896   \n",
      "41 : 40.44922   0.09090    65.79759   0.09864   \n",
      "42 : 40.45096   0.09047    65.82155   0.09797   \n",
      "43 : 40.44503   0.09020    65.81395   0.09794   \n"
     ]
    }
   ],
   "source": [
    "import torch as tc\n",
    "from torch.autograd import grad\n",
    "\n",
    "def test(dataloader, model, loss_fn, normalizer, device='cuda'):\n",
    "        \n",
    "    lossE, lossG, NTA = 0., 0., 0\n",
    "    model.eval()\n",
    "    for batch, _ in enumerate(dataloader):\n",
    "\n",
    "        symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients  = _\n",
    "#        with torch.no_grad():\n",
    "        _, pred, predG = model(symbols, positions, cells, pbcs, energyidx, crystalidx)\n",
    "    \n",
    "        loss = loss_fn(normalizer.denorm(pred), predG, energies, gradients)\n",
    "        A = len(gradients)\n",
    "        NTA += A\n",
    "        lossE += loss_fn.lossE.item()\n",
    "        lossG += loss_fn.lossG.item()\n",
    "\n",
    "    return lossE / NTA, lossG / NTA\n",
    "    \n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer, normalizer, device='cuda'):\n",
    "    \n",
    "    lossE, lossG, NTA = 0., 0., 0\n",
    "    model.train()\n",
    "    for batch, _ in enumerate(dataloader):\n",
    "\n",
    "        symbols, positions, energies, cells, pbcs, energyidx, crystalidx, gradients  = _\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        _, pred, predG = model(symbols, positions, cells, pbcs, energyidx, crystalidx)\n",
    "        \n",
    "        #loss = loss_fn(pred, predG, normalizer.norm(tc.tensor(energies)), gradients)\n",
    "        loss = loss_fn(pred, predG, normalizer.norm(energies), gradients)\n",
    "        \n",
    "        loss.requires_grad_(True)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        A = len(gradients)\n",
    "        NTA += A\n",
    "        lossE += loss_fn.lossE.item()\n",
    "        lossG += loss_fn.lossG.item()\n",
    "\n",
    "    return lossE / NTA, lossG / NTA\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def run(symbol, number, moduledict=None, modulelist=None, lmax=2, nmax=15, rcut=6., loop=2, batch_size=30, \n",
    "        location=\"/home01/x2419a03/libCalc/mlip/data/\", \n",
    "        device='cpu'):\n",
    "    species = [number]\n",
    "    \n",
    "    log_dir = './20220803/' + symbol + '_log'\n",
    "    \n",
    "    model = gen_module(species=species, moduledict=moduledict, modulelist=modulelist, \n",
    "                       lmax=lmax, nmax=nmax, loop=loop, rcut=rcut, device=device)\n",
    "#    model = nn.DataParallel(model)\n",
    "    ####\n",
    "#    model.load_state_dict(tc.load('/scratch/x2419a03/workspace/20220803/Cu_weights_710.pt'))\n",
    "#    model.desc.rcut = tc.tensor([rcut]).double().to(device=device)\n",
    "    ####\n",
    "    \n",
    "    imgdataset, dataloader = get_dataloader(location, symbol, number, 'training.json', batch_size=batch_size)\n",
    "    test_imgdataset, test_dataloader = get_dataloader(location, symbol, number, 'test.json', batch_size=batch_size)\n",
    "\n",
    "\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    normalizer = Normalizer(tc.tensor(imgdataset.energies).double())\n",
    "\n",
    "    lr, muF = 1e-2, 100\n",
    "    for t in range(5000):\n",
    "        loss_fn = MSEFLoss(muF=muF)\n",
    "        lossE, lossG = train(dataloader, model, loss_fn, \n",
    "                             tc.optim.Adam(model.parameters(), lr=lr), normalizer)\n",
    "        if t % 10 == 0:\n",
    "            tc.save(model.state_dict(), './20220803/' + symbol + '_weights_%d.pt' % t)\n",
    "        \n",
    "        # 50 meV\n",
    "        if lossG < 5e-3:\n",
    "            lr = 1e-5\n",
    "            muF = 1\n",
    "        # 150 meV\n",
    "        elif lossG < 1.5e-2:\n",
    "            lr = 1e-4\n",
    "            muF = 3\n",
    "        # 500 meV\n",
    "        elif lossG < 5e-2:\n",
    "            lr = 1e-3\n",
    "            muF = 10\n",
    "\n",
    "        writer.add_scalar('training RMSE-E (eV/atom)', lossE, t)\n",
    "        writer.add_scalar('training RMSE-F (eV/A)', lossG, t)\n",
    "        \n",
    "        test_lossE, test_lossG = test(test_dataloader, model, loss_fn, normalizer)\n",
    "        writer.add_scalar('test RMSE-E (eV/atom)', test_lossE, t)\n",
    "        writer.add_scalar('test RMSE-F (eV/A)', test_lossG, t)\n",
    "        \n",
    "        print(t, \": {0:<10.5f} {1:<10.5f} {2:<10.5f} {3:<10.5f}\".format(lossE, lossG, test_lossE, test_lossG))\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    tc.cuda.empty_cache()\n",
    "    \n",
    "device = 'cuda'\n",
    "#tables = {'Cu': 29, 'Ge': 32, 'Li': 3, 'Mo': 42, 'Ni':28, 'Si': 14}\n",
    "tables = {'Cu': 29}\n",
    "\n",
    "\n",
    "from mlip.reann import Gj\n",
    "\n",
    "device='cuda'\n",
    "desc_NO = 4\n",
    "species = [0]\n",
    "nmax = 20\n",
    "loop = 3\n",
    "rcut = 6.\n",
    "\n",
    "moduledict = nn.ModuleDict()\n",
    "for spe in species:\n",
    "    moduledict[str(spe)] = nn.Sequential(\n",
    "        nn.Linear(desc_NO, 128),\n",
    "        nn.Softplus(),\n",
    "        nn.Linear(128, 1),\n",
    "    )\n",
    "    \n",
    "modulelist = nn.ModuleList()\n",
    "for j in range(loop):\n",
    "    descdict = nn.ModuleDict()\n",
    "    for spe in species:\n",
    "        descdict[str(spe)] = nn.Sequential(\n",
    "                nn.Linear(desc_NO, 128),\n",
    "                 nn.Softplus(),\n",
    "                nn.Linear(128, nmax)\n",
    "        )\n",
    "    modulelist.append(Gj(descdict, species=species, nmax=nmax))\n",
    "\n",
    "for sym, num in tables.items():\n",
    "    run(sym, num, device=device, nmax=nmax, loop=loop, rcut=rcut,\n",
    "        moduledict=moduledict.double().to(device=device),\n",
    "        modulelist=modulelist.double().to(device=device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hello I am JaeHwan shim from jaejun Yu's group. \n",
    "\n",
    "I am going to explain about the Recursive embdding atomic nerual network. Recursive embedding neural network is neural network interatomic potential model which has foreground on the physics. \n",
    "\n",
    "Contents of my presentation is\n",
    "\n",
    "1. REANN 적용 현황\n",
    "\n",
    "single atoms\n",
    "\n",
    "\n",
    "\n",
    "Results \n",
    "\n",
    "Results and discussion \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'Cu': 29, 'Ge': 32, 'Li': 3, 'Mo': 42, 'Ni':28, 'Si': 14"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "simple",
   "language": "python",
   "name": "simple"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
